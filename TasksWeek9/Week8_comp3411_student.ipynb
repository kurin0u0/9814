{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695609c-f987-451f-92b2-92401333f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Section 1.a ###############\n",
    "\n",
    "greetGrammar = \"\"\"\n",
    "  S -> \"hello\" | \"hi\" | \"good to see you\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995f007-b650-489c-8d98-7ab0d4d51d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 1.b ####################### \n",
    "import nltk\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "grammar = nltk.CFG.fromstring(greetGrammar)\n",
    "print([g for g in generate(grammar)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe8dce-726a-4b6f-b12f-e8850a5507c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 1.c ####################### \n",
    "\n",
    "greetGrammarWithNames = \"\"\"\n",
    "############# TO DO PART ##################\n",
    "\"\"\"\n",
    "\n",
    "grammar = nltk.CFG.fromstring(greetGrammarWithNames)\n",
    "print([g for g in generate(grammar)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800302c-c271-4d2b-8f78-3dc89542eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 1.d ####################### \n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "positive_film_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "############# TO DO PART #############\n",
    "\"\"\")\n",
    "\n",
    "positive_reviews = list(generate(positive_film_grammar))\n",
    "for i in range(5):\n",
    "  print(random.choice(positive_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3ebe6-08fd-4311-9d15-172cc95b0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 1.e ####################### \n",
    "\n",
    "positive_film_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "##### TO DO PART ####\n",
    "\"\"\")\n",
    "\n",
    "positive_reviews = [' '.join(s) for s in generate(positive_film_grammar)]\n",
    "for i in range(5):\n",
    "  print(random.choice(positive_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4195d1-4d40-4821-8c4b-3d71f7253d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 1.f ####################### \n",
    "\n",
    "negative_film_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "##### TO DO PART ####\n",
    "\"\"\")\n",
    "\n",
    "negative_reviews = [' '.join(s) for s in generate(negative_film_grammar)]\n",
    "for i in range(5):\n",
    "  print(random.choice(negative_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f15c08-8f16-4673-963d-e665b41a3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 1.g ####################### \n",
    "\n",
    "grammar_training_dataset = []\n",
    "for i in range(1000):\n",
    "  positive_utterance = f\"{random.choice(positive_reviews)}. {random.choice(positive_reviews)}. {random.choice(positive_reviews)}\"\n",
    "  ###### TO DO #############  \n",
    "\n",
    "for i in range(5):\n",
    "  print(random.choice(grammar_training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb04e3-0c8d-4f90-ad67-55db0005ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 2.1.a ####################### \n",
    "\n",
    "import regex as re\n",
    "\n",
    "input_text = \"We hold these truths to be self-evident, that all men are created equal, that all men are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness and mental stability.\"\n",
    "\n",
    "re.sub(r\"\\bmen\\b\", \"people\", input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3211ba-11c6-4470-8f0a-9767ad5a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 2.1.b ####################### \n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk_data = []\n",
    "for file_id in movie_reviews.fileids():\n",
    "  nltk_data.append((movie_reviews.raw(file_id), movie_reviews.categories(file_id)[0]))\n",
    "\n",
    "print(len(nltk_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413eca5-26e7-4ce7-be96-fac1665f1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 2.1.c ####################### \n",
    "\n",
    "import regex as re\n",
    "\n",
    "def cleanup_review(review):\n",
    "############## TO DO PART ###############\n",
    "  # 1. Replace all URLs with URLTOKEN\n",
    "  # 2. Replace all dates with DATETOKEN\n",
    "  # 3. Remove all non-alphanumerical characters\n",
    "  # 4. Collapse multiple spaces into one space\n",
    "  return review\n",
    "\n",
    "\n",
    "cleanup_review(\"Hello!! My name is Stefano, I have been a tutor for COMP~9414 since 01/04/2023. My personal website is http://stefano.com . (Nice to meet you ^__^)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bc89f-5d2c-4471-bb39-26b93f92f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 2.2 ####################### \n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(999)\n",
    "\n",
    "cleanup_data = [(cleanup_review(r), l) for r,l in nltk_data]\n",
    "\n",
    "np.random.shuffle(cleanup_data)\n",
    "\n",
    "train_nltk_data = cleanup_data[0:int(len(cleanup_data)*0.85)]\n",
    "test_nltk_data = cleanup_data[int(len(cleanup_data)*0.85):int(len(cleanup_data)*0.95)]\n",
    "valid_nltk_data = cleanup_data[int(len(cleanup_data)*0.95):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32876d5f-64d9-417b-a8f2-ed4df8a69c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.a ####################### \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#### TO DO #########\n",
    "pipeline = Pipeline([\n",
    "  ('vect', ...),\n",
    "  ('clf', ...),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5adc86-b3ad-4a11-9ebb-de2f03613825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.b ####################### \n",
    "\n",
    "pipeline.fit([d[0] for d in grammar_training_dataset], [d[1] for d in grammar_training_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec08676-17ee-413b-b895-8dae951c8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.c ####################### \n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "y_predicted = pipeline.predict([t[0] for t in test_nltk_data])\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report([t[1] for t in test_nltk_data], y_predicted,\n",
    "                                    target_names=['positive', 'negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25ea7e-bdd9-473d-a094-e3a27a24fd9e",
   "metadata": {},
   "source": [
    "Here the results might not be very good as you trained your pipeline with grammar_training_dataset and you are testing it with test_nltk_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ee7f0-47b0-440a-952c-4bfa04f04496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.d ####################### \n",
    "\n",
    "####### TO DO ###########\n",
    "pipeline = Pipeline([....])\n",
    "\n",
    "pipeline.fit([d[0] for d in train_nltk_data], [d[1] for d in train_nltk_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334d982-0cb2-48a5-8371-2a63a6536ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.e ####################### \n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "###### TO DO ######\n",
    "y_predicted = \n",
    "# Print the classification report\n",
    "print(metrics.classification_report(....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b25c4-19e0-45ed-9f8a-a48c3bc9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.f ####################### \n",
    "# 3.f (i) grammar_training_dataset and nltk_data combined\n",
    "\n",
    "###### TO DO ######\n",
    "pipeline=Pipeline([...])\n",
    "pipeline.fit([....])\n",
    "y_predicted = pipeline.predict([...])\n",
    "print(metrics.classification_report([...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af51edb-e576-4e81-984a-59f6c51b738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Section 3.f ####################### \n",
    "# 3.f (ii) classifiers experiments\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "###### TO DO ######\n",
    "for c in classifiers:\n",
    "  print(f\"Training classifier model: {c.__str__()}\")\n",
    "  pipeline=Pipeline([....])\n",
    "  pipeline.fit(np.array([...]))\n",
    "  y_predicted = pipeline.predict([...])\n",
    "  print(metrics.classification_report([...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b9f5d-43c6-4ebd-b02d-87ec7912d9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
