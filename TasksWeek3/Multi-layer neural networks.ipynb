{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0afe6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'la' from 'tensorflow.keras' (/opt/anaconda3/envs/comp_9814/lib/python3.10/site-packages/keras/_tf_keras/keras/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m la\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'la' from 'tensorflow.keras' (/opt/anaconda3/envs/comp_9814/lib/python3.10/site-packages/keras/_tf_keras/keras/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094179a0",
   "metadata": {},
   "source": [
    "读compoundA.txt(神经模型中意思是First input)，substrate.txt(神经模型中意思是Second input)，biomass.txt（神经模型中意思是Output），数据都是一列，2000行，分析提供的数据并执行任何必要的预处理。\n",
    "预处理期间的一些任务可能包括下面所示的任务(a) 确定输入和输出变量的变化范围。(b) 绘制每个变量以观察生物过程的整体行为。(c) 如果检测到异常值，相应地修正数据。例如，因为我们处理的是以克为单位的变量，所以任何值都不应小于零。一个简单的修正方法是用零值替换这些值。(d) 将数据分成两个子集：训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f39af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Loading files ==\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to read compoundARaw.txt: read_csv() got an unexpected keyword argument 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36mread_1col_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'squeeze'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(log_lines))\n\u001b[1;32m     82\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m s_compound \u001b[38;5;241m=\u001b[39m \u001b[43mread_1col_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_COMPOUND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m s_substrate \u001b[38;5;241m=\u001b[39m read_1col_file(FILE_SUBSTRATE)\n\u001b[1;32m     86\u001b[0m s_biomass \u001b[38;5;241m=\u001b[39m read_1col_file(FILE_BIOMASS)\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mread_1col_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     67\u001b[0m     s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Ensure numeric\u001b[39;00m\n\u001b[1;32m     71\u001b[0m s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(s\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m s\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m s, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to read compoundARaw.txt: read_csv() got an unexpected keyword argument 'squeeze'"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\"\"\"\n",
    "Preprocessing script for three 1-column text files:\n",
    " - compoundA.txt (First input)\n",
    " - substrate.txt (Second input)\n",
    " - biomass.txt (Output)\n",
    "\n",
    "Assumptions:\n",
    " - Files are plain text, one numeric value per line (2000 rows expected).\n",
    " - Units are grams, so negative values are invalid and will be replaced with 0.\n",
    "\n",
    "What this script does:\n",
    " 1. Reads the three files into a pandas DataFrame.\n",
    " 2. Checks lengths and reports missing/extra rows.\n",
    " 3. Computes and prints summary statistics (min, max, mean, median, std).\n",
    " 4. Plots time-series, histograms, and boxplots for each variable and saves PNGs.\n",
    " 5. Detects negative values and replaces them with zero.\n",
    " 6. Detects outliers using the IQR method and (optionally) caps them or replaces with median.\n",
    " 7. Splits data into train/validation sets (default 80/20) with random_state for reproducibility.\n",
    " 8. Saves processed DataFrames and split files to disk.\n",
    "\n",
    "Usage:\n",
    " 1) Put this script in the same folder as compoundA.txt, substrate.txt, biomass.txt\n",
    " 2) `pip install pandas matplotlib scikit-learn` if you don't have them\n",
    " 3) Run: `python preprocess_neuro_data.py`\n",
    "\n",
    "Outputs saved to ./preprocessing_out/:\n",
    " - summary.txt            : text summary of statistics and steps\n",
    " - plots/*.png            : timeseries, histogram, boxplot for each variable\n",
    " - processed_full.csv     : combined dataframe after cleaning\n",
    " - train.csv / val.csv    : the train/validation split\n",
    "\n",
    "You can change options in the SETTINGS section below.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------- SETTINGS ----------------\n",
    "FILE_COMPOUND = \"compoundARaw.txt\"\n",
    "FILE_SUBSTRATE = \"substrateRaw.txt\"\n",
    "FILE_BIOMASS = \"biomassRaw.txt\"\n",
    "OUT_DIR = \"preprocessing_out\"\n",
    "PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "TRAIN_SIZE = 0.8\n",
    "RANDOM_STATE = 42\n",
    "APPLY_OUTLIER_CAP = True  # If True, cap outliers using IQR method; otherwise only report\n",
    "OUTLIER_CAP_METHOD = \"median\"  # \"median\" or \"clip\"\n",
    "# ------------------------------------------\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "log_lines = []\n",
    "\n",
    "def log(s):\n",
    "    print(s)\n",
    "    log_lines.append(s)\n",
    "\n",
    "\n",
    "def read_1col_file(path):\n",
    "    \"\"\"Read a single-column numeric file into a pandas Series.\"\"\"\n",
    "    try:\n",
    "        s = pd.read_csv(path, header=None, squeeze=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read {path}: {e}\")\n",
    "    # Ensure numeric\n",
    "    s = pd.to_numeric(s.iloc[:,0] if s.ndim>1 else s, errors='coerce')\n",
    "    return s\n",
    "\n",
    "# 1. Load\n",
    "log(\"== Loading files ==\")\n",
    "for f in [FILE_COMPOUND, FILE_SUBSTRATE, FILE_BIOMASS]:\n",
    "    if not os.path.exists(f):\n",
    "        log(f\"ERROR: required file '{f}' not found in current directory ({os.getcwd()}).\")\n",
    "        log(\"Place the files in the working directory and re-run this script.\")\n",
    "        with open(os.path.join(OUT_DIR, 'summary.txt'), 'w') as fh:\n",
    "            fh.write('\\n'.join(log_lines))\n",
    "        sys.exit(1)\n",
    "\n",
    "s_compound = read_1col_file(FILE_COMPOUND)\n",
    "s_substrate = read_1col_file(FILE_SUBSTRATE)\n",
    "s_biomass = read_1col_file(FILE_BIOMASS)\n",
    "\n",
    "log(f\"Loaded: {FILE_COMPOUND} ({len(s_compound)} rows), {FILE_SUBSTRATE} ({len(s_substrate)} rows), {FILE_BIOMASS} ({len(s_biomass)} rows)\")\n",
    "\n",
    "# 2. Align lengths\n",
    "N = max(len(s_compound), len(s_substrate), len(s_biomass))\n",
    "log(f\"Maximum row count across files = {N}\")\n",
    "\n",
    "# If sizes differ, we'll align by index: shorter series will get NaNs appended\n",
    "s_compound = s_compound.reset_index(drop=True).reindex(range(N))\n",
    "s_substrate = s_substrate.reset_index(drop=True).reindex(range(N))\n",
    "s_biomass = s_biomass.reset_index(drop=True).reindex(range(N))\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'compoundA': s_compound,\n",
    "    'substrate': s_substrate,\n",
    "    'biomass': s_biomass\n",
    "})\n",
    "\n",
    "log(\"\\n== Initial summary (including NaNs) ==\")\n",
    "log(df.describe(include='all').to_string())\n",
    "\n",
    "# 3. Determine ranges and missing values\n",
    "ranges = {}\n",
    "for col in df.columns:\n",
    "    col_nonnull = df[col].dropna()\n",
    "    ranges[col] = {\n",
    "        'count': int(col_nonnull.count()),\n",
    "        'min': float(col_nonnull.min()) if not col_nonnull.empty else np.nan,\n",
    "        'max': float(col_nonnull.max()) if not col_nonnull.empty else np.nan,\n",
    "        'mean': float(col_nonnull.mean()) if not col_nonnull.empty else np.nan,\n",
    "        'median': float(col_nonnull.median()) if not col_nonnull.empty else np.nan,\n",
    "        'std': float(col_nonnull.std()) if not col_nonnull.empty else np.nan,\n",
    "        'n_nan': int(df[col].isna().sum())\n",
    "    }\n",
    "\n",
    "log('\\nRanges and counts:')\n",
    "for k,v in ranges.items():\n",
    "    log(f\"{k}: count={v['count']}, n_nan={v['n_nan']}, min={v['min']}, max={v['max']}, mean={v['mean']}, median={v['median']}, std={v['std']}\")\n",
    "\n",
    "# 4. Plot each variable: time-series, histogram, boxplot\n",
    "log('\\n== Generating plots ==')\n",
    "for col in df.columns:\n",
    "    col_data = df[col]\n",
    "    idx = np.arange(len(col_data))\n",
    "\n",
    "    # Timeseries (line)\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(idx, col_data.values)\n",
    "    plt.title(f\"{col} - timeseries\")\n",
    "    plt.xlabel('index')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(PLOTS_DIR, f\"{col}_timeseries.png\")\n",
    "    plt.savefig(out)\n",
    "    plt.close()\n",
    "    log(f\"Saved {out}\")\n",
    "\n",
    "    # Histogram\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(col_data.dropna().values, bins=50)\n",
    "    plt.title(f\"{col} - histogram\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('count')\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(PLOTS_DIR, f\"{col}_hist.png\")\n",
    "    plt.savefig(out)\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.boxplot(col_data.dropna().values, vert=False)\n",
    "    plt.title(f\"{col} - boxplot\")\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(PLOTS_DIR, f\"{col}_boxplot.png\")\n",
    "    plt.savefig(out)\n",
    "    plt.close()\n",
    "\n",
    "log('Plots generated in ' + PLOTS_DIR)\n",
    "\n",
    "# 5. Fix physically impossible negatives (replace with 0)\n",
    "log('\\n== Fixing negative values (replace with 0) ==')\n",
    "neg_counts = {}\n",
    "for col in df.columns:\n",
    "    neg = (df[col] < 0).sum()\n",
    "    neg_counts[col] = int(neg)\n",
    "    if neg>0:\n",
    "        log(f\"{col}: found {neg} negative values -> replacing with 0\")\n",
    "        df.loc[df[col] < 0, col] = 0\n",
    "\n",
    "if all(v==0 for v in neg_counts.values()):\n",
    "    log('No negative values found.')\n",
    "\n",
    "# 6. Outlier detection with IQR\n",
    "log('\\n== Outlier detection (IQR method) ==')\n",
    "outlier_info = {}\n",
    "for col in df.columns:\n",
    "    s = df[col].dropna()\n",
    "    Q1 = s.quantile(0.25)\n",
    "    Q3 = s.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    mask_out = (s < lower) | (s > upper)\n",
    "    n_out = int(mask_out.sum())\n",
    "    outlier_info[col] = dict(Q1=float(Q1), Q3=float(Q3), IQR=float(IQR), lower=float(lower), upper=float(upper), n_out=n_out)\n",
    "    log(f\"{col}: Q1={Q1}, Q3={Q3}, IQR={IQR}, lower={lower}, upper={upper}, n_out={n_out}\")\n",
    "\n",
    "# Optionally cap or replace outliers\n",
    "if APPLY_OUTLIER_CAP:\n",
    "    log('\\nApplying outlier correction...')\n",
    "    for col in df.columns:\n",
    "        info = outlier_info[col]\n",
    "        lower = info['lower']\n",
    "        upper = info['upper']\n",
    "        if OUTLIER_CAP_METHOD == 'median':\n",
    "            median = df[col].median()\n",
    "            n_replaced = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "            df.loc[(df[col] < lower) | (df[col] > upper), col] = median\n",
    "            log(f\"{col}: replaced {int(n_replaced)} outliers with median={median}\")\n",
    "        elif OUTLIER_CAP_METHOD == 'clip':\n",
    "            before = df[col].copy()\n",
    "            df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "            n_changed = (before != df[col]).sum()\n",
    "            log(f\"{col}: clipped {int(n_changed)} values to [{lower}, {upper}]\")\n",
    "        else:\n",
    "            log(f\"Unknown OUTLIER_CAP_METHOD='{OUTLIER_CAP_METHOD}' - skipping outlier correction\")\n",
    "\n",
    "# 7. Final summary after cleaning\n",
    "log('\\n== Final summary after cleaning ==')\n",
    "for col in df.columns:\n",
    "    col_nonnull = df[col].dropna()\n",
    "    log(f\"{col}: count={int(col_nonnull.count())}, min={float(col_nonnull.min())}, max={float(col_nonnull.max())}, mean={float(col_nonnull.mean())}, median={float(col_nonnull.median())}, std={float(col_nonnull.std())}\")\n",
    "\n",
    "# 8. Split into train/validation\n",
    "log('\\n== Splitting into train and validation sets ==')\n",
    "# We'll split rows — ensure we drop rows with any NaNs first or optionally impute\n",
    "n_rows_before = len(df)\n",
    "rows_with_nan = df.isna().any(axis=1).sum()\n",
    "log(f\"Rows total = {n_rows_before}, rows with any NaN = {rows_with_nan}\")\n",
    "\n",
    "# Drop rows with NaNs for simplicity (report to user)\n",
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "log(f\"After dropping NaN rows: {len(df_clean)} rows remain\")\n",
    "\n",
    "X = df_clean[['compoundA', 'substrate']]\n",
    "y = df_clean['biomass']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=TRAIN_SIZE, random_state=RANDOM_STATE)\n",
    "train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "val = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)\n",
    "log(f\"Train rows: {len(train)}, Validation rows: {len(val)}\")\n",
    "\n",
    "# 9. Save outputs\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "processed_csv = os.path.join(OUT_DIR, 'processed_full.csv')\n",
    "train_csv = os.path.join(OUT_DIR, 'train.csv')\n",
    "val_csv = os.path.join(OUT_DIR, 'val.csv')\n",
    "\n",
    "df.to_csv(processed_csv, index=False)\n",
    "train.to_csv(train_csv, index=False)\n",
    "val.to_csv(val_csv, index=False)\n",
    "\n",
    "log(f\"Saved processed data to {processed_csv}, {train_csv}, {val_csv}\")\n",
    "\n",
    "# Save textual summary\n",
    "with open(os.path.join(OUT_DIR, 'summary.txt'), 'w') as fh:\n",
    "    fh.write('\\n'.join(log_lines))\n",
    "\n",
    "log('\\nPreprocessing complete. Review the plots and summary.txt in the preprocessing_out folder.')\n",
    "\n",
    "# Quick tips for the user on next steps\n",
    "log('\\nNext steps suggestions:')\n",
    "log('- Inspect the timeseries plots to understand temporal patterns or sensor drift.')\n",
    "log('- If you believe NaNs represent sensor dropout rather than missing rows, consider interpolation instead of dropping.')\n",
    "log('- For modeling, consider scaling features (StandardScaler or MinMaxScaler) depending on the chosen algorithm.')\n",
    "log('- If you want different outlier handling (e.g., winsorize, robust scaling), change APPLY_OUTLIER_CAP/OUTLIER_CAP_METHOD at the top and re-run.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_9814",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
