{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6cbc7df1-2c8f-48de-ac96-d5b3a8db5b7f",
   "metadata": {},
   "source": [
    "# Defining the environment and the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e5664-c640-4ee1-817c-b5011c71b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class World(object):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.R = np.zeros(self.x*self.y)\n",
    "        self.agentPos = 0\n",
    "        self.grid = np.zeros((self.x, self.y))  # Adding the grid so we can visualize the environment later\n",
    "        self.goalState = None  # Goal state will be set when calling setReward()\n",
    "        self.fearState= None\n",
    "\n",
    "    def xy2idx(self,x,y): # transfering the location to a unique id representing the current state\n",
    "        return x*self.y + y\n",
    "\n",
    "    def idx2xy(self,idx):\n",
    "        # To do\n",
    "        # Based on idx that we have, how we can retrieve x, y?\n",
    "        return x, y\n",
    "        \n",
    "\n",
    "    def resetAgent(self, pos):\n",
    "        self.agentPos = int(pos)\n",
    "\n",
    "    def setReward(self, x, y, r):\n",
    "        # get the goal state location, and assigne a reward to that cell\n",
    "        goalState = self.xy2idx(x, y)\n",
    "        self.R[goalState] = r\n",
    "        if r> 0:\n",
    "            self.goalState = (x, y)  # Store goal state as a tuple (x, y)\n",
    "        else:\n",
    "            self.fearState= (x,y)\n",
    "\n",
    "    def getState(self):\n",
    "        return self.agentPos\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.R[self.agentPos]\n",
    "\n",
    "    def getNumOfStates(self):\n",
    "        return self.x*self.y\n",
    "\n",
    "    def getNumOfActions(self):\n",
    "        return 4\n",
    "\n",
    "    def move(self,id):\n",
    "        x_, y_ = self.idx2xy(self.agentPos)\n",
    "        tmpX = x_\n",
    "        tmpY = y_\n",
    "        \n",
    "        # To do \n",
    "        # based on each possible action (id), change tmpX and tmpY\n",
    "\n",
    "        if self.validMove(tmpX, tmpY):\n",
    "            self.agentPos = self.xy2idx(tmpX,tmpY)\n",
    "\n",
    "    def validMove(self,x,y):\n",
    "        valid = True\n",
    "        # To do\n",
    "        # Add situations that Move will be invalid and Valid= False        \n",
    "        return valid\n",
    "\n",
    "\n",
    "    def display(self):\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        ax.imshow(self.grid, cmap=\"viridis\", origin=\"upper\", extent=(0, self.y, 0, self.x), alpha=0)\n",
    "        agent_x, agent_y = self.idx2xy(self.agentPos)  \n",
    "        agent_circle = plt.Circle((agent_y + 0.5, self.x - agent_x - 0.5), 0.3, color='grey', ec='black') \n",
    "        ax.add_patch(agent_circle) \n",
    "    \n",
    "\n",
    "        if hasattr(self, 'goalState'):\n",
    "            goal_x, goal_y = self.goalState \n",
    "            goal_circle = plt.Circle((goal_y + 0.5, self.x - goal_x - 0.5), 0.3, color='green', ec='black') \n",
    "            ax.add_patch(goal_circle)  \n",
    "    \n",
    " \n",
    "        if self.fearState != None:\n",
    "            if hasattr(self, 'fearState'):\n",
    "                fear_x, fear_y = self.fearState  \n",
    "                fear_circle = plt.Circle((fear_y + 0.5, self.x - fear_x - 0.5), 0.3, color='red', ec='black')  \n",
    "                ax.add_patch(fear_circle) \n",
    "        \n",
    "        ax.set_xticks(np.arange(self.y))\n",
    "        ax.set_yticks(np.arange(self.x))\n",
    "        ax.set_xticklabels(np.arange(self.y))\n",
    "        ax.set_yticklabels(np.arange(self.x)[::-1])\n",
    "        ax.grid(which=\"both\", color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a3d5f-a3de-453c-a55a-643ba198676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World(3,4)\n",
    "world.setReward(2, 3, 1.0) #Goal state\n",
    "world.setReward(1, 1, -1.0) #Fear region\n",
    "world.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d682ba-3c90-4a9a-8a25-87d006bcf75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a5b9723b-0141-4525-a7c5-d0d4dc51ea04",
   "metadata": {},
   "source": [
    "# Defining the Agent_SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6878ae0-5900-4306-9d24-9e3e639b92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_SARSA(object):\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "        self.numOfActions = self.world.getNumOfActions()\n",
    "        self.numOfStates = self.world.getNumOfStates()\n",
    "        self.Q = np.random.uniform(0.0,0.01,(self.numOfStates,self.numOfActions))\n",
    "        self.alpha = 0.7\n",
    "        self.gamma = 0.4\n",
    "        self.epsilon = 0.25\n",
    "\n",
    "    # epsilon-greedy action selection\n",
    "    def actionSelection(self, state):\n",
    "         # To do\n",
    "        # Add the algorithm that the agent choose an action based on epsilon-greedy action selection    \n",
    "        return action\n",
    "\n",
    "    def train(self, iter):\n",
    "        for itr in range(iter):\n",
    "\n",
    "            state = int(np.random.randint(0,self.numOfStates))\n",
    "            self.world.resetAgent(state)\n",
    "\n",
    "            # choose action\n",
    "            a = self.actionSelection(state)\n",
    "            expisode = True\n",
    "\n",
    "            while expisode:\n",
    "                # perform action\n",
    "                self.world.move(a)\n",
    "                # look for reward\n",
    "                reward = self.world.getReward()\n",
    "                state_new = int(self.world.getState())\n",
    "                # new action\n",
    "                a_new = self.actionSelection(state_new)\n",
    "\n",
    "                # To do\n",
    "                # calculate q value  and do the updating\n",
    "\n",
    "                if reward == 1.0:\n",
    "                    self.Q[state_new,:] = 0\n",
    "                    expisode = False\n",
    "\n",
    "        print(self.Q)\n",
    "        return self.Q\n",
    "\n",
    "    def plotQValues(self):\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        plt.imshow(self.Q, cmap='Oranges', interpolation='nearest', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Q-values\")\n",
    "        plt.xlabel(\"Actions\")\n",
    "        plt.ylabel(\"States\")\n",
    "        plt.xticks(np.arange(4), ('Down', 'Up', 'Right', 'Left'))\n",
    "        plt.yticks(np.arange(self.numOfStates), np.arange(self.numOfStates))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a730ca-7263-4b84-989c-ee360b524880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 3.a.\n",
    "world = World(3,4)\n",
    "\n",
    "#Section 3.b.\n",
    "world.setReward(2, 3, 1.0) #Goal state\n",
    "world.setReward(1, 1, -1.0) #Fear region\n",
    "\n",
    "#Section 3.c.\n",
    "learner_SARSA = Agent_SARSA(world)\n",
    "\n",
    "#Section 3.d.\n",
    "learner_SARSA.train(1000)\n",
    "\n",
    "#Section 3.e.\n",
    "learner_SARSA.plotQValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e11b5-2242-4452-b5d3-6da37553e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_path(agent, world):\n",
    "    done = True\n",
    "    steps = 0\n",
    "\n",
    "    # Start with a random state\n",
    "    state = int(np.random.randint(0, agent.numOfStates))\n",
    "    world.resetAgent(state)\n",
    "\n",
    "    while done:\n",
    "        world.display() \n",
    "        \n",
    "        #To do\n",
    "        # Choose the action based on the Q-table\n",
    "        print('Action taken:', )\n",
    "        print('Q-table values for all actions in the state:',)\n",
    "        world.move()\n",
    "\n",
    "        # Get reward and new state\n",
    "        reward = world.getReward()\n",
    "        state = int(world.getState())\n",
    "        steps += 1\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if reward == 1.0:\n",
    "            done = False\n",
    "\n",
    "    world.display()  # Final visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0b904-1b4d-4c50-8774-b75835776187",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_path(learner, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c81a4-2118-47cb-8c53-bf11c501544d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "779cc945-4b96-45e1-ae11-2fe185cce695",
   "metadata": {},
   "source": [
    "Extra activity: Can you develope an agent works based on Q-learning? Can you identify the difference between SARSA and \n",
    "Q-learning algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614f4c1-7576-4550-8e1e-1591486d7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Q_learning(object):\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "        self.numOfActions = self.world.getNumOfActions()\n",
    "        self.numOfStates = self.world.getNumOfStates()\n",
    "        self.Q = np.random.uniform(0.0, 0.01, (self.numOfStates, self.numOfActions))\n",
    "        self.alpha = 0.7\n",
    "        self.gamma = 0.4\n",
    "        self.epsilon = 0.25\n",
    "\n",
    "    # epsilon-greedy action selection\n",
    "    def actionSelection(self, state):\n",
    "         # To do\n",
    "        # Add the algorithm that the agent choose an action based on that    \n",
    "        return action\n",
    "\n",
    "    def train(self, iter):\n",
    "        for itr in range(iter):\n",
    "            state = int(np.random.randint(0, self.numOfStates))\n",
    "            self.world.resetAgent(state)\n",
    "\n",
    "            expisode = True\n",
    "\n",
    "            while expisode:\n",
    "                # choose action\n",
    "                a = self.actionSelection(state)\n",
    "\n",
    "                # perform action\n",
    "                self.world.move(a)\n",
    "\n",
    "                # look for reward\n",
    "                reward = self.world.getReward()\n",
    "                state_new = int(self.world.getState())\n",
    "                \n",
    "                #To do\n",
    "                # update Q-values (using max Q-value for next state)\n",
    "\n",
    "\n",
    "                if reward == 1.0:\n",
    "                    self.Q[state_new,:] = 0\n",
    "                    expisode = False\n",
    "\n",
    "        print(self.Q)\n",
    "        return self.Q\n",
    "\n",
    "    def plotQValues(self):\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        plt.imshow(self.Q, cmap='Oranges', interpolation='nearest', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Q-values\")\n",
    "        plt.xlabel(\"Actions\")\n",
    "        plt.ylabel(\"States\")\n",
    "        plt.xticks(np.arange(4), ('Down', 'Up', 'Right', 'Left'))\n",
    "        plt.yticks(np.arange(self.numOfStates), np.arange(self.numOfStates))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba86353-b65c-420e-a428-6de811a1fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "world = World(3,4)\n",
    "\n",
    "#Section 3.b.\n",
    "world.setReward(2, 3, 1.0) #Goal state\n",
    "world.setReward(1, 1, -1.0) #Fear region\n",
    "\n",
    "#Section 3.c.\n",
    "learner_Q_learning = Agent_Q_learning(world)\n",
    "\n",
    "#Section 3.d.\n",
    "learner_Q_learning.train(1000)\n",
    "\n",
    "#Section 3.e.\n",
    "learner_Q_learning.plotQValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba5590-0007-4ba9-8dff-65ba1225db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_path(learner_Q_learning , world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d562cca-e69b-4199-8e91-0e31b1b18b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
