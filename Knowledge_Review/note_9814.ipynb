{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ffd4ad",
   "metadata": {},
   "source": [
    "# week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755a42e",
   "metadata": {},
   "source": [
    "1.1.Agent\n",
    "    Reactive Agent反应：基于当前感知行动，路径可能次优. \n",
    "    Model-based Agent基于模型：跟踪看不到事物来处理部分可观测性，但不能计划未来，实现例如自动驾驶/扫地机器人. \n",
    "    Planning Agent规划：与条件行动不同，有未来后果考虑，需要搜索，灵活易变但反应慢. \n",
    "        例如：Goal-based Agent基于目标：地图搜索，象棋. \n",
    "    Learning Agent学习：包含performance element:读sensors来行动/critic:给反馈/Learning element:用②确定修改①/problem generator:设新任务，给信息。众模块非孤立. \n",
    "  \n",
    "1.2.Search：策略不同在于扩大边界方式. \n",
    "\n",
    "1.2.1.uninformed：  \n",
    "    BFS广度：  \n",
    "    DFS深度：堆栈（除iddfs外其他都队列）. \n",
    "    IDDFS迭代深化深度：不超给定深度迭代dfs. \n",
    "    UCS统一成本：选path弧的和最少，成本相同变bfs. \n",
    "\n",
    "1.2.2.informed知情搜索：启发式，用domain knowledge，朝最佳猜测目标行. \n",
    "    GBFS贪婪：选最低heuristic cost. \n",
    "    A*:统一成本+贪婪  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296cc99",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11d472",
   "metadata": {},
   "source": [
    "2.1.Supervised Learning 监督学习\n",
    "    An equation relating input to output\n",
    "    Search through family of possible equations to find one that fits training data well\n",
    "\n",
    "2.1.1.Regression回归\n",
    "    单变量回归问题（一个输出，实数值）\n",
    "\n",
    "2.1.2.Binary Classification 二元分类\n",
    "    二元分类问题（两个离散类别）\n",
    "\n",
    "2.1.3.Multiclass Classification\n",
    "    多类分类问题（离散类别，>2 个可能的值）\n",
    "\n",
    "2.2.Unsupervised Learning 无监督学习\n",
    "    学习无标签数据集\n",
    "    Clustering 聚类：将相似的数据点分组在一起\n",
    "\n",
    "2.3.Reinforcement Learning 强化学习\n",
    "    一组 States 状态/ Actions 动作/ Rewards 奖励\n",
    "    Goal: take actions to change the state so that you receive rewards\n",
    "    You have to explore the environment yourself to gather data as you go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d0917",
   "metadata": {},
   "source": [
    "2.4.Decision Trees 决策树（supervised machine learning）\n",
    "    应用: classification and regression\n",
    "    partition the data into subsets that are increasingly homogeneous同质 with respect to the response variable\n",
    "    Feature A may be the most important feature(它将训练样本划分为更接近完全正面或完全负面的子集（即更同质化）)，so check it earlier(倒着画)\n",
    "    Goal:generalizes泛化 well from the training data and accurately classifies previously unseen samples准确分类先前未见过的样本\n",
    "\n",
    "2.4.1.Entropy 熵 (randomness or uncertainty 随机性或不确定性)\n",
    "    𝐻(⟨p1,· · · , pn ⟩) = sigma(i=1,n) −p𝑖log2𝑝𝑖\n",
    "    maximized when all outcomes are equally likely\n",
    "    minimized when the probability distribution is highly concentrated around a single outcome\n",
    "    决策树通过在每个步骤中选择【最小】熵的特征来构建\n",
    "\n",
    "2.4.2.Minimal Error Pruning 最小错误剪枝\n",
    "    prune branches that do not provide much benefit in classifying the items (aids generalization, avoids overfitting)\n",
    "    Laplace error 拉普拉斯误差:E= 1−(n+1)/(N+k)\n",
    "        N = total number of (training) items at the node\n",
    "        n = number of (training) items in the majority class\n",
    "        k = number of classes\n",
    "    如果子节点的平均拉普拉斯误差超过父节点，则剪除子节点\n",
    "\n",
    "2.4.3.Tree Limitations\n",
    "    high variance高方差(不稳定): 1.arise from a tiny change in the training data, leading to completely different predictions\n",
    "                               2.模型倾向于“记忆”训练数据（overfitting 过拟合），而不是学习潜在的模式\n",
    "                               3.低方差: 模型在不同数据集上更加稳定和一致\n",
    "    Suboptimal predictive performance 预测性能不佳: Single trees often fail to achieve strong generalization 泛化能力\n",
    "    generalization 泛化 是指机器学习模型在未见过数据上表现良好，而不仅仅是训练数据\n",
    "    divided into three subsets: training, validation,and test\n",
    "                              1.训练集：用于训练决策树模型。\n",
    "                              2.验证集：用于在训练过程中评估不同的配置（超参数）\n",
    "                              3.测试集：为最终决策树模型在未见过数据上提供无偏评估。\n",
    "    Bias 偏差: systematic error 系统性误差, 用简单的模型近似现实问题时引入的\n",
    "        A high bias model makes strong assumptions about the data, leading to underfitting欠拟合\n",
    "    bias-variance tradeoff:复杂的模型可以帮助我们避免偏差（欠拟合），而限制模型复杂度可以帮助我们避免过拟合（高方差）\n",
    "\n",
    "2.4.4.Tree Depth\n",
    "    Shallow trees 浅树由于树没有足够的节点来捕捉数据的复杂性，因此会遭受高偏差（欠拟合）\n",
    "    Deep trees 深树容易受到高方差（过拟合）的影响，因为树可能会为每个数据样本分配一个独特的路径，而不是学习一般模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882d542",
   "metadata": {},
   "source": [
    "2.5.Ensemble Learning with Trees 基于树的集成学习\n",
    "2.5.1.Bagged Tree 装袋树\n",
    "    relies on bootstrapping 自助采样 to create multiple training datasets\n",
    "    bootstrapping: with replacement 有放回, sample is the same size 规模相同 as the original dataset\n",
    "        37% out-of-bag (OOB) samples: 未包含在自助采样中的数据点\n",
    "    集成中的每个模型都会为新样本生成一个预测，最终预测是通过the majority vote rule多数投票规则（用于分类）或通过平均预测值（用于回归）来做出的\n",
    "    Advantages:1.reduces the variance\n",
    "               2.average prediction has lower variance\n",
    "               3.errors(vary in different directions) partially cancel out\n",
    "               4.No separate test set required\n",
    "               5.OOB samples provides an unbiased estimate of performance 无偏的性能估计\n",
    "    每棵子树最终都是独一无二的,然而，这些树彼此之间仍然存在一定的correlated相关性,因此，装袋方法实现的方差减少可以进一步改进，从统计学的角度来看，通过在树构建过程中引入additional randomness额外的随机性，可以减少预测变量之间的相关性\n",
    "\n",
    "2.5.2.Random Forest 随机森林\n",
    "    最终预测: the majority vote rule\n",
    "    在每个树的分割点，算法随机选择 k predictors (features) k 个预测器（特征）。k = √p，其中 p 是特征的总数\n",
    "    然后，树仅从这些 k 个特征中选择最佳分割\n",
    "    这种随机性减少了树之间的相关性（这是装袋树的问题）\n",
    "    与装袋法相比，随机森林在逐棵树的基础上更具有计算效率，因为树构建过程仅需在每个分裂点评估a fraction of the original features原始特征的一部分，尽管随机森林通常需要more trees更多的树\n",
    "    选择随机森林和 Bagging 中树的数量（m）：1.Variance reduction 方差减少: 增加更多的树可以减少方差，使预测更加稳定\n",
    "                                      2.Constraints 约束条件: 当增加 m 时，训练时间、内存使用和过拟合是主要的限制因素\n",
    "                                      3.Test error behavior 测试误差行为: 测试误差通常随着 m 的增加而单调递减——起初迅速下降，然后趋于平稳，在足够多的树之后几乎保持恒定\n",
    "\n",
    "2.5.3.Boosting Trees(AdaBoost)提升树\n",
    "    每次迭代中，AdaBoost 根据当前的加权数据点选择最佳分类器\n",
    "    在 k 次迭代中被错误分类的数据点在(k+1)次迭代中获得higher weights更高的权重\n",
    "    因此，难以分类的样本会逐渐获得越来越大的权重\n",
    "    这个过程确保每次迭代都专注于学习数据的某个不同方面\n",
    "    最后，这些weighted classifiers加权分类器的序列被组合成一个集成，产生一个强大的整体模型\n",
    "    Algorithm:计算\n",
    "\n",
    "2.5.4.对比\n",
    "    装袋法通过在自助采样上训练多个树并聚合它们的预测来降低high variance高方差\n",
    "    随机森林通过在训练中添加randomness随机性（例如，在每个分裂时选择随机特征子集）进一步variance reduction减少方差.树是in parallel并行且同等权重训练的，这能有效减少方差，但并不能显著reduce bias减少偏差\n",
    "    提升方法通过sequentially顺序训练树来减少bias and variance偏差和方差，其中每棵新树专注于纠正前一棵树的错误，并根据其性能对树进行加权\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f342b",
   "metadata": {},
   "source": [
    "# Week3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c3740",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
