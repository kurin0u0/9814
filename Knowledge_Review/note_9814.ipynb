{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ffd4ad",
   "metadata": {},
   "source": [
    "week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755a42e",
   "metadata": {},
   "source": [
    "1.1.Agent\n",
    "Reactive Agent反应：基于当前感知行动，路径可能次优. \n",
    "Model-based Agent基于模型：跟踪看不到事物来处理部分可观测性，但不能计划未来，实现例如自动驾驶/扫地机器人. \n",
    "Planning Agent规划：与条件行动不同，有未来后果考虑，需要搜索，灵活易变但反应慢. \n",
    "例如：Goal-based Agent基于目标：地图搜索，象棋. \n",
    "Learning Agent学习：包含performance element:读sensors来行动/critic:给反馈/Learning element:用②确定修改①/problem generator:设新任务，给信息。众模块非孤立. \n",
    "  \n",
    "1.2.Search：策略不同在于扩大边界方式. \n",
    "1.2.1.uninformed：  \n",
    "BFS广度：  \n",
    "DFS深度：堆栈（除iddfs外其他都队列）. \n",
    "IDDFS迭代深化深度：不超给定深度迭代dfs. \n",
    "UCS统一成本：选path弧的和最少，成本相同变bfs. \n",
    "1.2.2.informed知情搜索：启发式，用domain knowledge，朝最佳猜测目标行. \n",
    "GBFS贪婪：选最低heuristic cost. \n",
    "A*:统一成本+贪婪  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
