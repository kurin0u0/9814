{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ffd4ad",
   "metadata": {},
   "source": [
    "# week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755a42e",
   "metadata": {},
   "source": [
    "1.1.Agent\n",
    "    Reactive Agentååº”ï¼šåŸºäºå½“å‰æ„ŸçŸ¥è¡ŒåŠ¨ï¼Œè·¯å¾„å¯èƒ½æ¬¡ä¼˜. \n",
    "    Model-based AgentåŸºäºæ¨¡å‹ï¼šè·Ÿè¸ªçœ‹ä¸åˆ°äº‹ç‰©æ¥å¤„ç†éƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼Œä½†ä¸èƒ½è®¡åˆ’æœªæ¥ï¼Œå®ç°ä¾‹å¦‚è‡ªåŠ¨é©¾é©¶/æ‰«åœ°æœºå™¨äºº. \n",
    "    Planning Agentè§„åˆ’ï¼šä¸æ¡ä»¶è¡ŒåŠ¨ä¸åŒï¼Œæœ‰æœªæ¥åæœè€ƒè™‘ï¼Œéœ€è¦æœç´¢ï¼Œçµæ´»æ˜“å˜ä½†ååº”æ…¢. \n",
    "        ä¾‹å¦‚ï¼šGoal-based AgentåŸºäºç›®æ ‡ï¼šåœ°å›¾æœç´¢ï¼Œè±¡æ£‹. \n",
    "    Learning Agentå­¦ä¹ ï¼šåŒ…å«performance element:è¯»sensorsæ¥è¡ŒåŠ¨/critic:ç»™åé¦ˆ/Learning element:ç”¨â‘¡ç¡®å®šä¿®æ”¹â‘ /problem generator:è®¾æ–°ä»»åŠ¡ï¼Œç»™ä¿¡æ¯ã€‚ä¼—æ¨¡å—éå­¤ç«‹. \n",
    "  \n",
    "1.2.Searchï¼šç­–ç•¥ä¸åŒåœ¨äºæ‰©å¤§è¾¹ç•Œæ–¹å¼. \n",
    "\n",
    "1.2.1.uninformedï¼š  \n",
    "    BFSå¹¿åº¦ï¼š  \n",
    "    DFSæ·±åº¦ï¼šå †æ ˆï¼ˆé™¤iddfså¤–å…¶ä»–éƒ½é˜Ÿåˆ—ï¼‰. \n",
    "    IDDFSè¿­ä»£æ·±åŒ–æ·±åº¦ï¼šä¸è¶…ç»™å®šæ·±åº¦è¿­ä»£dfs. \n",
    "    UCSç»Ÿä¸€æˆæœ¬ï¼šé€‰pathå¼§çš„å’Œæœ€å°‘ï¼Œæˆæœ¬ç›¸åŒå˜bfs. \n",
    "\n",
    "1.2.2.informedçŸ¥æƒ…æœç´¢ï¼šå¯å‘å¼ï¼Œç”¨domain knowledgeï¼Œæœæœ€ä½³çŒœæµ‹ç›®æ ‡è¡Œ. \n",
    "    GBFSè´ªå©ªï¼šé€‰æœ€ä½heuristic cost. \n",
    "    A*:ç»Ÿä¸€æˆæœ¬+è´ªå©ª  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296cc99",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11d472",
   "metadata": {},
   "source": [
    "2.1.Supervised Learning ç›‘ç£å­¦ä¹ \n",
    "    An equation relating input to output\n",
    "    Search through family of possible equations to find one that fits training data well\n",
    "\n",
    "2.1.1.Regressionå›å½’\n",
    "    å•å˜é‡å›å½’é—®é¢˜ï¼ˆä¸€ä¸ªè¾“å‡ºï¼Œå®æ•°å€¼ï¼‰\n",
    "\n",
    "2.1.2.Binary Classification äºŒå…ƒåˆ†ç±»\n",
    "    äºŒå…ƒåˆ†ç±»é—®é¢˜ï¼ˆä¸¤ä¸ªç¦»æ•£ç±»åˆ«ï¼‰\n",
    "\n",
    "2.1.3.Multiclass Classification\n",
    "    å¤šç±»åˆ†ç±»é—®é¢˜ï¼ˆç¦»æ•£ç±»åˆ«ï¼Œ>2 ä¸ªå¯èƒ½çš„å€¼ï¼‰\n",
    "\n",
    "2.2.Unsupervised Learning æ— ç›‘ç£å­¦ä¹ \n",
    "    å­¦ä¹ æ— æ ‡ç­¾æ•°æ®é›†\n",
    "    Clustering èšç±»ï¼šå°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„åœ¨ä¸€èµ·\n",
    "\n",
    "2.3.Reinforcement Learning å¼ºåŒ–å­¦ä¹ \n",
    "    ä¸€ç»„ States çŠ¶æ€/ Actions åŠ¨ä½œ/ Rewards å¥–åŠ±\n",
    "    Goal: take actions to change the state so that you receive rewards\n",
    "    You have to explore the environment yourself to gather data as you go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d0917",
   "metadata": {},
   "source": [
    "2.4.Decision Trees å†³ç­–æ ‘ï¼ˆsupervised machine learningï¼‰\n",
    "    åº”ç”¨: classification and regression\n",
    "    partition the data into subsets that are increasingly homogeneousåŒè´¨ with respect to the response variable\n",
    "    Feature A may be the most important feature(å®ƒå°†è®­ç»ƒæ ·æœ¬åˆ’åˆ†ä¸ºæ›´æ¥è¿‘å®Œå…¨æ­£é¢æˆ–å®Œå…¨è´Ÿé¢çš„å­é›†ï¼ˆå³æ›´åŒè´¨åŒ–ï¼‰)ï¼Œso check it earlier(å€’ç€ç”»)\n",
    "    Goal:generalizesæ³›åŒ– well from the training data and accurately classifies previously unseen sampleså‡†ç¡®åˆ†ç±»å…ˆå‰æœªè§è¿‡çš„æ ·æœ¬\n",
    "\n",
    "2.4.1.Entropy ç†µ (randomness or uncertainty éšæœºæ€§æˆ–ä¸ç¡®å®šæ€§)\n",
    "    ğ»(âŸ¨p1,Â· Â· Â· , pn âŸ©) = sigma(i=1,n) âˆ’pğ‘–log2ğ‘ğ‘–\n",
    "    maximized when all outcomes are equally likely\n",
    "    minimized when the probability distribution is highly concentrated around a single outcome\n",
    "    å†³ç­–æ ‘é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é€‰æ‹©ã€æœ€å°ã€‘ç†µçš„ç‰¹å¾æ¥æ„å»º\n",
    "\n",
    "2.4.2.Minimal Error Pruning æœ€å°é”™è¯¯å‰ªæ\n",
    "    prune branches that do not provide much benefit in classifying the items (aids generalization, avoids overfitting)\n",
    "    Laplace error æ‹‰æ™®æ‹‰æ–¯è¯¯å·®:E= 1âˆ’(n+1)/(N+k)\n",
    "        N = total number of (training) items at the node\n",
    "        n = number of (training) items in the majority class\n",
    "        k = number of classes\n",
    "    å¦‚æœå­èŠ‚ç‚¹çš„å¹³å‡æ‹‰æ™®æ‹‰æ–¯è¯¯å·®è¶…è¿‡çˆ¶èŠ‚ç‚¹ï¼Œåˆ™å‰ªé™¤å­èŠ‚ç‚¹\n",
    "\n",
    "2.4.3.Tree Limitations\n",
    "    high varianceé«˜æ–¹å·®(ä¸ç¨³å®š): 1.arise from a tiny change in the training data, leading to completely different predictions\n",
    "                               2.æ¨¡å‹å€¾å‘äºâ€œè®°å¿†â€è®­ç»ƒæ•°æ®ï¼ˆoverfitting è¿‡æ‹Ÿåˆï¼‰ï¼Œè€Œä¸æ˜¯å­¦ä¹ æ½œåœ¨çš„æ¨¡å¼\n",
    "                               3.ä½æ–¹å·®: æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šæ›´åŠ ç¨³å®šå’Œä¸€è‡´\n",
    "    Suboptimal predictive performance é¢„æµ‹æ€§èƒ½ä¸ä½³: Single trees often fail to achieve strong generalization æ³›åŒ–èƒ½åŠ›\n",
    "    generalization æ³›åŒ– æ˜¯æŒ‡æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè€Œä¸ä»…ä»…æ˜¯è®­ç»ƒæ•°æ®\n",
    "    divided into three subsets: training, validation,and test\n",
    "                              1.è®­ç»ƒé›†ï¼šç”¨äºè®­ç»ƒå†³ç­–æ ‘æ¨¡å‹ã€‚\n",
    "                              2.éªŒè¯é›†ï¼šç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°ä¸åŒçš„é…ç½®ï¼ˆè¶…å‚æ•°ï¼‰\n",
    "                              3.æµ‹è¯•é›†ï¼šä¸ºæœ€ç»ˆå†³ç­–æ ‘æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šæä¾›æ— åè¯„ä¼°ã€‚\n",
    "    Bias åå·®: systematic error ç³»ç»Ÿæ€§è¯¯å·®, ç”¨ç®€å•çš„æ¨¡å‹è¿‘ä¼¼ç°å®é—®é¢˜æ—¶å¼•å…¥çš„\n",
    "        A high bias model makes strong assumptions about the data, leading to underfittingæ¬ æ‹Ÿåˆ\n",
    "    bias-variance tradeoff:å¤æ‚çš„æ¨¡å‹å¯ä»¥å¸®åŠ©æˆ‘ä»¬é¿å…åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼Œè€Œé™åˆ¶æ¨¡å‹å¤æ‚åº¦å¯ä»¥å¸®åŠ©æˆ‘ä»¬é¿å…è¿‡æ‹Ÿåˆï¼ˆé«˜æ–¹å·®ï¼‰\n",
    "\n",
    "2.4.4.Tree Depth\n",
    "    Shallow trees æµ…æ ‘ç”±äºæ ‘æ²¡æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹æ¥æ•æ‰æ•°æ®çš„å¤æ‚æ€§ï¼Œå› æ­¤ä¼šé­å—é«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰\n",
    "    Deep trees æ·±æ ‘å®¹æ˜“å—åˆ°é«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰çš„å½±å“ï¼Œå› ä¸ºæ ‘å¯èƒ½ä¼šä¸ºæ¯ä¸ªæ•°æ®æ ·æœ¬åˆ†é…ä¸€ä¸ªç‹¬ç‰¹çš„è·¯å¾„ï¼Œè€Œä¸æ˜¯å­¦ä¹ ä¸€èˆ¬æ¨¡å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882d542",
   "metadata": {},
   "source": [
    "2.5.Ensemble Learning with Trees åŸºäºæ ‘çš„é›†æˆå­¦ä¹ \n",
    "2.5.1.Bagged Tree è£…è¢‹æ ‘\n",
    "    relies on bootstrapping è‡ªåŠ©é‡‡æ · to create multiple training datasets\n",
    "    bootstrapping: with replacement æœ‰æ”¾å›, sample is the same size è§„æ¨¡ç›¸åŒ as the original dataset\n",
    "        37% out-of-bag (OOB) samples: æœªåŒ…å«åœ¨è‡ªåŠ©é‡‡æ ·ä¸­çš„æ•°æ®ç‚¹\n",
    "    é›†æˆä¸­çš„æ¯ä¸ªæ¨¡å‹éƒ½ä¼šä¸ºæ–°æ ·æœ¬ç”Ÿæˆä¸€ä¸ªé¢„æµ‹ï¼Œæœ€ç»ˆé¢„æµ‹æ˜¯é€šè¿‡the majority vote ruleå¤šæ•°æŠ•ç¥¨è§„åˆ™ï¼ˆç”¨äºåˆ†ç±»ï¼‰æˆ–é€šè¿‡å¹³å‡é¢„æµ‹å€¼ï¼ˆç”¨äºå›å½’ï¼‰æ¥åšå‡ºçš„\n",
    "    Advantages:1.reduces the variance\n",
    "               2.average prediction has lower variance\n",
    "               3.errors(vary in different directions) partially cancel out\n",
    "               4.No separate test set required\n",
    "               5.OOB samples provides an unbiased estimate of performance æ— åçš„æ€§èƒ½ä¼°è®¡\n",
    "    æ¯æ£µå­æ ‘æœ€ç»ˆéƒ½æ˜¯ç‹¬ä¸€æ— äºŒçš„,ç„¶è€Œï¼Œè¿™äº›æ ‘å½¼æ­¤ä¹‹é—´ä»ç„¶å­˜åœ¨ä¸€å®šçš„correlatedç›¸å…³æ€§,å› æ­¤ï¼Œè£…è¢‹æ–¹æ³•å®ç°çš„æ–¹å·®å‡å°‘å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›ï¼Œä»ç»Ÿè®¡å­¦çš„è§’åº¦æ¥çœ‹ï¼Œé€šè¿‡åœ¨æ ‘æ„å»ºè¿‡ç¨‹ä¸­å¼•å…¥additional randomnessé¢å¤–çš„éšæœºæ€§ï¼Œå¯ä»¥å‡å°‘é¢„æµ‹å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§\n",
    "\n",
    "2.5.2.Random Forest éšæœºæ£®æ—\n",
    "    æœ€ç»ˆé¢„æµ‹: the majority vote rule\n",
    "    åœ¨æ¯ä¸ªæ ‘çš„åˆ†å‰²ç‚¹ï¼Œç®—æ³•éšæœºé€‰æ‹© k predictors (features) k ä¸ªé¢„æµ‹å™¨ï¼ˆç‰¹å¾ï¼‰ã€‚k = âˆšpï¼Œå…¶ä¸­ p æ˜¯ç‰¹å¾çš„æ€»æ•°\n",
    "    ç„¶åï¼Œæ ‘ä»…ä»è¿™äº› k ä¸ªç‰¹å¾ä¸­é€‰æ‹©æœ€ä½³åˆ†å‰²\n",
    "    è¿™ç§éšæœºæ€§å‡å°‘äº†æ ‘ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆè¿™æ˜¯è£…è¢‹æ ‘çš„é—®é¢˜ï¼‰\n",
    "    ä¸è£…è¢‹æ³•ç›¸æ¯”ï¼Œéšæœºæ£®æ—åœ¨é€æ£µæ ‘çš„åŸºç¡€ä¸Šæ›´å…·æœ‰è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºæ ‘æ„å»ºè¿‡ç¨‹ä»…éœ€åœ¨æ¯ä¸ªåˆ†è£‚ç‚¹è¯„ä¼°a fraction of the original featuresåŸå§‹ç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼Œå°½ç®¡éšæœºæ£®æ—é€šå¸¸éœ€è¦more treesæ›´å¤šçš„æ ‘\n",
    "    é€‰æ‹©éšæœºæ£®æ—å’Œ Bagging ä¸­æ ‘çš„æ•°é‡ï¼ˆmï¼‰ï¼š1.Variance reduction æ–¹å·®å‡å°‘: å¢åŠ æ›´å¤šçš„æ ‘å¯ä»¥å‡å°‘æ–¹å·®ï¼Œä½¿é¢„æµ‹æ›´åŠ ç¨³å®š\n",
    "                                      2.Constraints çº¦æŸæ¡ä»¶: å½“å¢åŠ  m æ—¶ï¼Œè®­ç»ƒæ—¶é—´ã€å†…å­˜ä½¿ç”¨å’Œè¿‡æ‹Ÿåˆæ˜¯ä¸»è¦çš„é™åˆ¶å› ç´ \n",
    "                                      3.Test error behavior æµ‹è¯•è¯¯å·®è¡Œä¸º: æµ‹è¯•è¯¯å·®é€šå¸¸éšç€ m çš„å¢åŠ è€Œå•è°ƒé€’å‡â€”â€”èµ·åˆè¿…é€Ÿä¸‹é™ï¼Œç„¶åè¶‹äºå¹³ç¨³ï¼Œåœ¨è¶³å¤Ÿå¤šçš„æ ‘ä¹‹åå‡ ä¹ä¿æŒæ’å®š\n",
    "\n",
    "2.5.3.Boosting Trees(AdaBoost)æå‡æ ‘\n",
    "    æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒAdaBoost æ ¹æ®å½“å‰çš„åŠ æƒæ•°æ®ç‚¹é€‰æ‹©æœ€ä½³åˆ†ç±»å™¨\n",
    "    åœ¨ k æ¬¡è¿­ä»£ä¸­è¢«é”™è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹åœ¨(k+1)æ¬¡è¿­ä»£ä¸­è·å¾—higher weightsæ›´é«˜çš„æƒé‡\n",
    "    å› æ­¤ï¼Œéš¾ä»¥åˆ†ç±»çš„æ ·æœ¬ä¼šé€æ¸è·å¾—è¶Šæ¥è¶Šå¤§çš„æƒé‡\n",
    "    è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿æ¯æ¬¡è¿­ä»£éƒ½ä¸“æ³¨äºå­¦ä¹ æ•°æ®çš„æŸä¸ªä¸åŒæ–¹é¢\n",
    "    æœ€åï¼Œè¿™äº›weighted classifiersåŠ æƒåˆ†ç±»å™¨çš„åºåˆ—è¢«ç»„åˆæˆä¸€ä¸ªé›†æˆï¼Œäº§ç”Ÿä¸€ä¸ªå¼ºå¤§çš„æ•´ä½“æ¨¡å‹\n",
    "    Algorithm:è®¡ç®—\n",
    "\n",
    "2.5.4.å¯¹æ¯”\n",
    "    è£…è¢‹æ³•é€šè¿‡åœ¨è‡ªåŠ©é‡‡æ ·ä¸Šè®­ç»ƒå¤šä¸ªæ ‘å¹¶èšåˆå®ƒä»¬çš„é¢„æµ‹æ¥é™ä½high varianceé«˜æ–¹å·®\n",
    "    éšæœºæ£®æ—é€šè¿‡åœ¨è®­ç»ƒä¸­æ·»åŠ randomnesséšæœºæ€§ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ¯ä¸ªåˆ†è£‚æ—¶é€‰æ‹©éšæœºç‰¹å¾å­é›†ï¼‰è¿›ä¸€æ­¥variance reductionå‡å°‘æ–¹å·®.æ ‘æ˜¯in parallelå¹¶è¡Œä¸”åŒç­‰æƒé‡è®­ç»ƒçš„ï¼Œè¿™èƒ½æœ‰æ•ˆå‡å°‘æ–¹å·®ï¼Œä½†å¹¶ä¸èƒ½æ˜¾è‘—reduce biaså‡å°‘åå·®\n",
    "    æå‡æ–¹æ³•é€šè¿‡sequentiallyé¡ºåºè®­ç»ƒæ ‘æ¥å‡å°‘bias and varianceåå·®å’Œæ–¹å·®ï¼Œå…¶ä¸­æ¯æ£µæ–°æ ‘ä¸“æ³¨äºçº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ï¼Œå¹¶æ ¹æ®å…¶æ€§èƒ½å¯¹æ ‘è¿›è¡ŒåŠ æƒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f342b",
   "metadata": {},
   "source": [
    "# Week3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c3740",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
