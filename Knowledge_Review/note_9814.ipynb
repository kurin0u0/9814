{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c95b42",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf854ab0",
   "metadata": {},
   "source": [
    "1.1.Agent\n",
    "    Reactive Agent反应：基于当前感知行动，路径可能次优. \n",
    "    Model-based Agent基于模型：跟踪看不到事物来处理部分可观测性，但不能计划未来，实现例如自动驾驶/扫地机器人. \n",
    "    Planning Agent规划：与条件行动不同，有未来后果考虑，需要搜索，灵活易变但反应慢. \n",
    "        例如：Goal-based Agent基于目标：地图搜索，象棋. \n",
    "    Learning Agent学习：包含performance element:读sensors来行动/critic:给反馈/Learning element:用②确定修改①/problem generator:设新任务，给信息。众模块非孤立. \n",
    "  \n",
    "1.2.Search：策略不同在于扩大边界方式. \n",
    "\n",
    "1.2.1.uninformed：  \n",
    "    BFS广度：  \n",
    "    DFS深度：堆栈（除iddfs外其他都队列）. \n",
    "    IDDFS迭代深化深度：不超给定深度迭代dfs. \n",
    "    UCS统一成本：选path弧的和最少，成本相同变bfs. \n",
    "\n",
    "1.2.2.informed知情搜索：启发式，用domain knowledge，朝最佳猜测目标行. \n",
    "    GBFS贪婪：选最低heuristic cost. \n",
    "    A*:统一成本+贪婪  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c188ec0",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfcf276",
   "metadata": {},
   "source": [
    "2.1.Supervised Learning 监督学习\n",
    "    An equation relating input to output\n",
    "    Search through family of possible equations to find one that fits training data well\n",
    "\n",
    "2.1.1.Regression回归\n",
    "    单变量回归问题（一个输出，实数值）\n",
    "\n",
    "2.1.2.Binary Classification 二元分类\n",
    "    二元分类问题（两个离散类别）\n",
    "\n",
    "2.1.3.Multiclass Classification\n",
    "    多类分类问题（离散类别，>2 个可能的值）\n",
    "\n",
    "2.2.Unsupervised Learning 无监督学习\n",
    "    学习无标签数据集\n",
    "    Clustering 聚类：将相似的数据点分组在一起\n",
    "\n",
    "2.3.Reinforcement Learning 强化学习\n",
    "    一组 States 状态/ Actions 动作/ Rewards 奖励\n",
    "    Goal: take actions to change the state so that you receive rewards\n",
    "    You have to explore the environment yourself to gather data as you go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001099e9",
   "metadata": {},
   "source": [
    "2.4.Decision Trees 决策树（supervised machine learning）\n",
    "    应用: classification and regression\n",
    "    partition the data into subsets that are increasingly homogeneous同质 with respect to the response variable\n",
    "    Feature A may be the most important feature(它将训练样本划分为更接近完全正面或完全负面的子集（即更同质化）)，so check it earlier(倒着画)\n",
    "    Goal:generalizes泛化 well from the training data and accurately classifies previously unseen samples准确分类先前未见过的样本\n",
    "\n",
    "2.4.1.Entropy 熵 (randomness or uncertainty 随机性或不确定性)\n",
    "    𝐻(⟨p1,· · · , pn ⟩) = sigma(i=1,n) −p𝑖log2𝑝𝑖\n",
    "    maximized when all outcomes are equally likely\n",
    "    minimized when the probability distribution is highly concentrated around a single outcome\n",
    "    决策树通过在每个步骤中选择【最小】熵的特征来构建\n",
    "\n",
    "2.4.2.Minimal Error Pruning 最小错误剪枝\n",
    "    prune branches that do not provide much benefit in classifying the items (aids generalization, avoids overfitting)\n",
    "    Laplace error 拉普拉斯误差:E= 1−(n+1)/(N+k)\n",
    "        N = total number of (training) items at the node\n",
    "        n = number of (training) items in the majority class\n",
    "        k = number of classes\n",
    "    如果子节点的平均拉普拉斯误差超过父节点，则剪除子节点\n",
    "\n",
    "2.4.3.Tree Limitations\n",
    "    high variance高方差(不稳定): 1.arise from a tiny change in the training data, leading to completely different predictions\n",
    "                               2.模型倾向于“记忆”训练数据（overfitting 过拟合），而不是学习潜在的模式\n",
    "                               3.低方差: 模型在不同数据集上更加稳定和一致\n",
    "    Suboptimal predictive performance 预测性能不佳: Single trees often fail to achieve strong generalization 泛化能力\n",
    "    generalization 泛化 是指机器学习模型在未见过数据上表现良好，而不仅仅是训练数据\n",
    "    divided into three subsets: training, validation,and test\n",
    "                              1.训练集：用于训练决策树模型。\n",
    "                              2.验证集：用于在训练过程中评估不同的配置（超参数）\n",
    "                              3.测试集：为最终决策树模型在未见过数据上提供无偏评估。\n",
    "    Bias 偏差: systematic error 系统性误差, 用简单的模型近似现实问题时引入的\n",
    "        A high bias model makes strong assumptions about the data, leading to underfitting欠拟合\n",
    "    bias-variance tradeoff:复杂的模型可以帮助我们避免偏差（欠拟合），而限制模型复杂度可以帮助我们避免过拟合（高方差）\n",
    "\n",
    "2.4.4.Tree Depth\n",
    "    Shallow trees 浅树由于树没有足够的节点来捕捉数据的复杂性，因此会遭受高偏差（欠拟合）\n",
    "    Deep trees 深树容易受到高方差（过拟合）的影响，因为树可能会为每个数据样本分配一个独特的路径，而不是学习一般模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad0483",
   "metadata": {},
   "source": [
    "2.5.Ensemble Learning with Trees 基于树的集成学习\n",
    "2.5.1.Bagged Tree 装袋树\n",
    "    relies on bootstrapping 自助采样 to create multiple training datasets\n",
    "    bootstrapping: with replacement 有放回, sample is the same size 规模相同 as the original dataset\n",
    "        37% out-of-bag (OOB) samples: 未包含在自助采样中的数据点\n",
    "    集成中的每个模型都会为新样本生成一个预测，最终预测是通过the majority vote rule多数投票规则（用于分类）或通过平均预测值（用于回归）来做出的\n",
    "    Advantages:1.reduces the variance\n",
    "               2.average prediction has lower variance\n",
    "               3.errors(vary in different directions) partially cancel out\n",
    "               4.No separate test set required\n",
    "               5.OOB samples provides an unbiased estimate of performance 无偏的性能估计\n",
    "    每棵子树最终都是独一无二的,然而，这些树彼此之间仍然存在一定的correlated相关性,因此，装袋方法实现的方差减少可以进一步改进，从统计学的角度来看，通过在树构建过程中引入additional randomness额外的随机性，可以减少预测变量之间的相关性\n",
    "\n",
    "2.5.2.Random Forest 随机森林\n",
    "    最终预测: the majority vote rule\n",
    "    在每个树的分割点，算法随机选择 k predictors (features) k 个预测器（特征）。k = √p，其中 p 是特征的总数\n",
    "    然后，树仅从这些 k 个特征中选择最佳分割\n",
    "    这种随机性减少了树之间的相关性（这是装袋树的问题）\n",
    "    与装袋法相比，随机森林在逐棵树的基础上更具有计算效率，因为树构建过程仅需在每个分裂点评估a fraction of the original features原始特征的一部分，尽管随机森林通常需要more trees更多的树\n",
    "    选择随机森林和 Bagging 中树的数量（m）：1.Variance reduction 方差减少: 增加更多的树可以减少方差，使预测更加稳定\n",
    "                                      2.Constraints 约束条件: 当增加 m 时，训练时间、内存使用和过拟合是主要的限制因素\n",
    "                                      3.Test error behavior 测试误差行为: 测试误差通常随着 m 的增加而单调递减——起初迅速下降，然后趋于平稳，在足够多的树之后几乎保持恒定\n",
    "\n",
    "2.5.3.Boosting Trees(AdaBoost)提升树\n",
    "    每次迭代中，AdaBoost 根据当前的加权数据点选择最佳分类器\n",
    "    在 k 次迭代中被错误分类的数据点在(k+1)次迭代中获得higher weights更高的权重\n",
    "    因此，难以分类的样本会逐渐获得越来越大的权重\n",
    "    这个过程确保每次迭代都专注于学习数据的某个不同方面\n",
    "    最后，这些weighted classifiers加权分类器的序列被组合成一个集成，产生一个强大的整体模型\n",
    "    Algorithm:计算\n",
    "\n",
    "\n",
    "2.5.4.对比\n",
    "    装袋法通过在自助采样上训练多个树并聚合它们的预测来降低high variance高方差\n",
    "    随机森林通过在训练中添加randomness随机性（例如，在每个分裂时选择随机特征子集）进一步variance reduction减少方差.树是in parallel并行且同等权重训练的，这能有效减少方差，但并不能显著reduce bias减少偏差\n",
    "    提升方法通过sequentially顺序训练树来减少bias and variance偏差和方差，其中每棵新树专注于纠正前一棵树的错误，并根据其性能对树进行加权\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45199a53",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93278383",
   "metadata": {},
   "source": [
    "3.1.Artificial Neural Networks人工神经网络\n",
    "    This model consists of many interconnected processing units (neurons)神经元 that work in parallel to accomplish a global task\n",
    "    model relationships between inputs and outputs or to discover patterns in data\n",
    "    Learning occurs by adapting weights, architecture, and activation/transfer functions to improve performance\n",
    "    Characterized by: Number of neurons, Interconnection architecture, Weight values, Activation and transfer functions\n",
    "\n",
    "3.1.1.model of a neuron\n",
    "    An extra constant w0 called the bias偏差 is also added\n",
    "\n",
    "3.1.2.Neural Networks are made up of nodes which have:\n",
    "    Input edges, each with some weight\n",
    "    Output edges (with weights)\n",
    "    An activation level (a function of the inputs)\n",
    "        Weights can be positive or negative and may change overtime (learning).\n",
    "        The input function is the weighted sum of the activation levels of inputs.\n",
    "        The activation level is a non-linear transfer function (activation function), g, of this input.\n",
    "        z= g(s)= g(𝑤0 + σ𝑖 𝑤𝑖𝑥𝑖) 必须大于0才是1\n",
    "\n",
    "3.2.Perceptron感知器\n",
    "    An artificial neuron with step transfer function阶跃传递函数(零点左右0,1) is called a Perceptron\n",
    "    The bias (w0) was thought of as a kind of threshold阈值. If the combination -σ𝑖 𝑤𝑖𝑥𝑖 is less than this threshold, the neuron would \"fire\" (its output would be 1)\n",
    "    The higher the bias, the more likely the neuron is to fire\n",
    "    Later on, alternative transfer functions were introduced which are continuous and (mostly) differentiable\n",
    "3.2.1.hyperplane\n",
    "    The weights and bias of a Perceptron define a hyperplane超平面 that divides the input space into two regions\n",
    "        For inputs on one side of the hyperplane, the output is 0\n",
    "        For inputs on the other side, the output is 1\n",
    "    Functions that can be computed in this way are called linearly separable线性可分. With this structure, a perceptron (artificial neuron) can learn any linear relationship任何线性关系\n",
    "3.2.2.Application\n",
    "    Limitation: What happens if the data is not linearly separable\n",
    "    AND, OR, and NOT become linearly separable,but XOR are not\n",
    "\n",
    "3.3.Multi Layer Perceptron多层感知器\n",
    "    compute XOR, one approach is to rewrite it in terms of linearly separable functions such as AND, OR, and NOR, and then arrange several perceptron多个感知器排列 into a network that combines these functions appropriately\n",
    "    However, in practice, we usually deal with raw data原始数据 rather than explicit logical expressions\n",
    "    perceptron learning algorithm—that can learn the weights of a neural network了解神经网络的权重 from a set of training examples\n",
    "\n",
    "3.4.Learning with Gradient Descent and Backpropagation通过梯度下降和反向传播进行学习\n",
    "    to optimize over a family of continuous and differentiable functions\n",
    "    define an error function误差函数 (also called a loss function or cost function) 𝐸as half the sum, over all input items, of the square of the difference between the actual output 𝑧𝑖and the target output 𝑡𝑖\n",
    "    goal is to find the minimum of the error function 𝐸找到误差函数 E 的最小值, and minimum of a function can be calculated through its derivative导数\n",
    "    We can use multi-variable derivative to adjust the weights in such a way as to take us in the steepest downhill direction. w ← w− η 𝝏𝑬/𝝏𝒘 , η is called the learning rate学习率\n",
    "    If we use the step function as the transfer function, the error landscape is not smooth; it consists almost entirely of flat regions and \"shoulders,\" with occasional discontinuous jumps.\n",
    "    For a single-layer perceptron, this was not a problem, but for networks with two or more layers, it becomes a major obstacle.\n",
    "    To apply gradient descent successfully, neural networks needed to be redesigned so that the function from input to output would be smooth and differentiable平滑且可微\n",
    "3.4.1.Learning rate identify the step size学习率确定步长\n",
    "    The key idea is to replace the (discontinuous) step function with a differentiable function, such as the sigmoid or hyperbolic tangent g(s)= 𝟏/𝟏+𝒆^(−𝒔)\n",
    "    We now describe how to compute the partial derivatives of the loss function with respect to each weight损失函数相对于每个权重的偏导数\n",
    "    For illustration, we consider a two-layer neural network with sigmoid activation激活 sigmoid 的两层神经网络 at the hidden layer, as shown in the diagram\n",
    "    𝑥𝑖 are the inputs, 𝑦𝑖 are the hidden units, 𝑤𝑖𝑗 and 𝑣𝑖 are the weights, and 𝑏𝑖 and 𝑐 are the biases\n",
    "    Chain Rule(会考)\n",
    "\n",
    "3.4.2.Backpropagation反向传播\n",
    "    Forward pass前向传递: apply inputs to the “lowest layer” and feed activations forward to get output\n",
    "    Calculate error计算误差: difference between desired output and actual output \n",
    "    Backward pass向后传递: Propagate errors back through the network to adjust weights\n",
    "    Since our prediction is greater than the target value, we need to decrease the weight降低权重 in order to reduce the prediction in the next iterations\n",
    "    As we propagate back to the weights in the earlier layers, the amount of adjustment (decrement) becomes smaller变小. For example, the change in 𝑤13 is more significant than the change in 𝑤11\n",
    "\n",
    "3.5.Neural Networks Design神经网络设计\n",
    "\n",
    "3.5.1.Exhaustive Analysis of the System对系统的详尽分析\n",
    "    Is a neural network really the best solution for this problem? Do I have the necessary requirements?神经网络真的是解决这个问题的最佳解决方案吗？我有必要的要求吗？\n",
    "    Neural Networks: The second-best option神经网络：第二好的选择\n",
    "        Neural networks are highly data-sensitive and usually require large datasets, since they need to learn a large number of parameters (𝑤𝑖)\n",
    "        Collecting and preparing such data can be costly and time-consuming\n",
    "\n",
    "3.5.2.Preprocessing预处理\n",
    "    What steps should I take before feeding data into the network在将数据输入网络之前，我应该采取哪些步骤？\n",
    "    Data数据:\n",
    "        A neural network is essentially a black-box model designed for interpolation (with no guarantee of good performance in extrapolation).\n",
    "        Therefore, its effectiveness strongly depends on the quality and quantity of the data available.\n",
    "    Quality质量:\n",
    "        This refers to how well the available data represents the underlying function being approximated.\n",
    "    Quantity数量:\n",
    "        Only with a sufficiently large dataset can we expect to correctly identify the parameters (weights) of a neural model.\n",
    "        If the available data is insufficient, data augmentation techniques can be used to expand it\n",
    "3.5.2.1.Data cleaning:\n",
    "    Detect and, if possible, eliminate outliers, empty values, etc. It might also help to detect correlations between variables.\n",
    "    Normalisation of variables:\n",
    "        Xn = (X- Xmin)/ (Xmax-Xmin); Xn ∈ [0,1] or Xn = 2*(X-Xmin)/(Xmax-Xmin) – 1; Xn ∈ [-1,1]\n",
    "    It is necessary to perform the corresponding denormalization at the output stage输出阶段\n",
    "3.5.2.2.why data normalization为什么要进行数据归一化\n",
    "    1. To ensure features are comparable in scale 确保功能在规模上具有可比性\n",
    "        Many datasets include features with very different ranges (e.g., age in years vs. income in dollars).\n",
    "        Without normalization, features with larger scales can dominate smaller ones in distance-based or gradient-based models.\n",
    "    2. To improve training stability and speed 提高训练稳定性和速度\n",
    "        Gradient descent converges faster when features are on a similar scale.\n",
    "        If not normalized, the loss landscape can become very steep in some directions and flat in others.\n",
    "\n",
    "3.5.3.Design of the Neural Network 神经网络的设计\n",
    "    What should the architecture of my network look like? (e.g., number of layers, number of neurons per layer, choice of activation functions, and other hyperparameters). 我的网络架构应该是什么样子？（例如，层数、每层神经元数、激活函数的选择和其他超参数）\n",
    "    Rule of thumb经验法则: Nh should lead to a number of parameters (weights) Nw that: Nw < (Number of samples) / 10\n",
    "        The number of weights Nw of a MLP, with Ni neurons in its input layer, a hidden layer with Nh neurons, and No neurons in the output layer is: Nw = (Ni+1)*Nh+(Nh+1)*No\n",
    "    Conclusion: more parameters and deeper networks do not lead to better a performance necessarily.\n",
    "    There should be a proportion between your dataset size, problem complexity, etc. and number of parameters 数据集大小、问题复杂性等与参数数量之间应该有一定比例\n",
    "    In MLPs is demonstrated that using one hidden layer with a proper number of neurons is sufficient to approximate any non-linear function with an arbitrary precision degree (Universal Approximation Theorem)在 MLP 中，证明使用具有适当数量神经元的隐藏层就足够以任意精度度近似任何非线性函数（通用逼近定理）。\n",
    "    Activation functions激活函数:\n",
    "        A usual criterion is to use sigmoid functions or ReLUs in the hidden layer and linear functions in the output. However, sigmoids or softmax can also be used in the output.\n",
    "\n",
    "3.5.4.Training\n",
    "    What happens during the training phase?\n",
    "    Training a neural network is a hard process due to the complexity of the error function solution space, which can have numerous local minima, minimax points, etc\n",
    "    There are three main problems that can arise during training:\n",
    "        Bias偏差\n",
    "        Overparameterization过度参数化\n",
    "        Overfitting过拟合\n",
    "    The latter two might affect the network's ability to generalize (high variance)泛化能力（高方差）\n",
    "3.5.4.1.Bias\n",
    "    refers to the systematic error introduced by approximating a real-world problem (which may be very complex) with a simpler model.偏差是指通过使用更简单的模型近似现实世界的问题（可能非常复杂）而引入的系统误差。\n",
    "    A high bias model makes strong assumptions about the data, leading to underfitting.高偏差模型对数据做出强烈的假设，导致拟合不足。\n",
    "    It can’t capture the true complexity of the underlying relationship它无法捕捉潜在关系的真正复杂性。\n",
    "    To Decrease Bias:\n",
    "        One way to reduce bias is to run multiple training processes starting from different randomly chosen initial weights (e.g., 20 or more attempts). This increases the chance of reaching a better local minimum.\n",
    "        Another approach is to increase the number of neurons in the hidden layer, which allows the model to better capture the complexity of the problem.\n",
    "            However, there is a trade-off: adding too many neurons can lead to high variance and overparameterization, causing the model to overfit添加过多的神经元会导致高方差和过度参数化，导致模型过度拟合。\n",
    "3.5.4.2.Overfitting过拟合\n",
    "    A model is considered overparameterized when it has more trainable parameters than the available training data can uniquely determine.\n",
    "    In simpler terms, the model has too many parameters relative to the size or complexity of the dataset.\n",
    "    As a result, it becomes powerful enough to memorize记住 the training data (including noise) rather than learning the underlying general patterns\n",
    "    The neural network is trained by presenting it with the input and target values for all items in the training set\n",
    "    After completing the learning procedure, it must then predict the outputs for items in the test set\n",
    "    The goal is to accurately predict the target values for the test set based on the input attributes.\n",
    "    A common mistake to avoid is building a model that fits the training data very well but performs poorly on unseen test data—this problem is known as overfitting.过度拟合\n",
    "    In contrast, a model that achieves high accuracy on both the training and test sets is said to have good generalization概括性\n",
    "    To determine the optimal model parameters, the dataset is often divided into Training, Validation, and Test (generalization) sets为了确定最佳模型参数，数据集通常分为训练集、验证集和测试（泛化）集。\n",
    "    Typically, as the number of hidden nodes increases, both the training and test errors initially decrease.\n",
    "    However, beyond a certain point, the training error may continue to decrease while the test error plateaus or even slightly increases. The approximate number of hidden nodes at which the test error is minimal is likely to achieve the best generalization performance测试误差最小的隐藏节点的近似数量可能会实现最佳的泛化性能\n",
    "3.5.4.3.Conclusion:\n",
    "    Increasing the number of neurons can help reduce bias (underfitting), while limiting the number of neurons helps prevent overfitting (high variance). This balance reflects the well-known bias–variance trade-off.增加神经元数量有助于减少偏差（欠拟合），而限制神经元数量有助于防止过度拟合（高方差）。这种平衡反映了众所周知的偏差-方差权衡。\n",
    "    In addition to adjusting the network size, there are other techniques available to address\n",
    "underfitting and overfitting, such as regularization, early stopping, and data augmentation.\n",
    "\n",
    "3.5.5.Testing and Evaluation测试和评估\n",
    "    How should I evaluate the performance of my network\n",
    "    To test the generalization capability of the network, performance is evaluated on the test set在测试集\n",
    "    Some common loss functions:\n",
    "        Mean Squared Error (MSE):\n",
    "        Mean Absolute Error (MAE):\n",
    "        Cross-Entropy (Binary Cross-Entropy (for 2 classes))\n",
    "\n",
    "3.6.Deep Learning 深度学习\n",
    "    If we denote the first layer as 𝑓 1 , the second layer is 𝑓 2 , and so on. The total number of layers defines the depth深度 of the model, which is why this approach is known as deep learning深度学习\n",
    "3.6.1.depth\n",
    "    The depth of a model is defined as the total number of layers with learnable parameters, excluding theinput layer.不包括输入层\n",
    "    Deeper networks can typically approximate more complex functions, while shallower models have fewer layers and are limited to approximating simpler functions.\n",
    "\n",
    "Deep learning is based on the philosophy of connectionism:连接主义\n",
    "    while an individual biological neuron is not particularly intelligent, a large population of neurons or features acting together can exhibit intelligent behavior\n",
    "    It is important to emphasize that the number of neurons must be large.神经元的数量必须很大\n",
    "    One of the key factors behind the dramatic improvement in neural network accuracy and the complexity of tasks they can handle—from the 1980s to today—is the significant increase in network size\n",
    "\n",
    "    Deep learning refers to artificial neural networks with multiple layers that automatically learn\n",
    "patterns and representations from data\n",
    "    What makes a network “deep” is the presence of many layers of neurons, not just one or two. A shallow network浅层网络 might have only a single hidden layer, whereas a deep network深度网络 can have hundreds or even thousands of layers\n",
    "\n",
    "    Universal Approximation Theorem通用逼近定理\n",
    "        One layer with many neurons:\n",
    "            wide hidden layer非常宽的隐藏层,\n",
    "            but it is still shallow深度较低 \n",
    "            computationally inefficient and does not capture hierarchical representations effectively (low precision)\n",
    "        Many layers, each with one neuron:\n",
    "            This network is extremely deep but very narrow.非常深，但非常狭窄\n",
    "            It qualifies as deep learning深度学习 because the depth (number of layers) is enormous.\n",
    "            However, with only one neuron per layer, the network’s expressive power is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c36d9",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdedcd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38c218e",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feeb715",
   "metadata": {},
   "source": [
    "5.1.Optimisation method 优化方法\n",
    "    dvantages of gradient descent:梯度下降的优点\n",
    "        Easy implementation.\n",
    "        Standard method that works generally well.\n",
    "    Drawbacks of gradient descent:梯度下降的缺点\n",
    "        Slow and inefficient.\n",
    "        It might get stuck on local minima leading to suboptimal results\n",
    "    Improvements to gradient descent:梯度下降的改进\n",
    "        Momentum 动量: Add percentage of last movement to the actual one.\n",
    "        Stochastic batch (SGD) 随机批次: Estimate gradient using subsample set.\n",
    "        Adaptable estimation (Adam) 适应估计: Adapt the learning rate for each weight of the neural network\n",
    "    The option to elegantly and efficiently calculate the gradient allows us to handle the optimisation problem using the full range of optimisation methods provided by nonlinear optimisation systems\n",
    "\n",
    "5.2.Asymptotic complexity 渐近复杂度\n",
    "    Complexity 复杂性: The complexity of an algorithm is a measure of the amount of resources it consumes.\n",
    "    Resource 资源:\n",
    "        Time\n",
    "        Space\n",
    "        Memory\n",
    "        Drive\n",
    "\n",
    "5.3.Classes of problems 问题类别\n",
    "5.3.1.Class P:\n",
    "        A P problem can be solved in polynomial time in a deterministic computer.\n",
    "        The class P comprises problems that can be solved quickly.\n",
    "        Examples: Quicksort, binary search, matrix multiplication.\n",
    "5.3.2.Class NP:\n",
    "        An NP problem cannot be solved in a polynomial time using a deterministic computer (intractable problem).\n",
    "        The class NP comprises problems whose solutions can be verified quickly (P⊆NP).\n",
    "        Examples: subsets addition, Sudoku, TSP.\n",
    "5.3.3.Class NP-complete:\n",
    "        A class of problems for which it is unknown if they are tractable.\n",
    "        No one has found polynomial algorithms for any of them.\n",
    "        Some problems closely related to tractable problems.\n",
    "        All known algorithms for NP-complete problems require exponential time in relation to the size of the input.\n",
    "5.3.4.algorithms to solve an NP-complete problem\n",
    "5.3.4.1.Approximation 近似:\n",
    "        An algorithm that quickly finds a solution that may not be optimal but falls within a certain range of error. In some cases, finding a good approximation is sufficient to solve the problem, but not all NP-complete problems have good approximation algorithms.\n",
    "5.3.4.2.Heuristics and Metaheuristics 启发式和元启发式:\n",
    "        An algorithm that performs reasonably well in many cases. They are generally fast, but there is no measure of the quality of the answer.\n",
    "5.3.4.3.Genetic Algorithms 遗传算法:\n",
    "        Algorithms that improve possible solutions until finding one that is possibly close to the optimum. There is also no guarantee of the quality of the answer.\n",
    "\n",
    "5.4.Search space搜索空间\n",
    "    Feasible solution: is in the feasible region of the problem.\n",
    "    Solution space S and objective function f.\n",
    "    The optimisation problem O(S, f) is solved by determining an optimal solution, i.e., a feasible solution x0 ∈ S / f(x) ≤ f(x0 ) Ɐ x ∈ S.\n",
    "    Constraints of the problem reduce the universe of solutions U, then X ⊆ U, also called feasible region.\n",
    "\n",
    "5.4.1.Neighbourhood search procedures 邻里搜查程序:\n",
    "    Transformations or movements from the current solution.\n",
    "        Generate an initial solution.\n",
    "        Iteratively modify it until a stopping criterion.\n",
    "        Solutions are evaluated while traversing.\n",
    "    Possible movements create a neighbourhood.\n",
    "    Feasible movements are those that provide a feasible solution.\n",
    "\n",
    "5.4.2.Constraints 约束:\n",
    "    Can be strong (must be satisfied) or weak (recommended to be satisfied).\n",
    "        Example: In course scheduling, a strong constraint is that classes should not overlap, while a weak constraint is that there should be no classes after 4pm.\n",
    "\n",
    "5.4.3.Restricted exploration of the feasible region within the search space 搜索空间内可行区域的有限探索:\n",
    "    Advantages: Infeasible solutions are not evaluated. The algorithms ensure obtaining a feasible solution.\n",
    "    Disadvantages: The search can be inefficient if restricted only to the feasible region. Optimal solutions may be located near the boundary and difficult to reach.\n",
    "\n",
    "5.4.4.Complete exploration of the solution space 完整探索解决方案空间:\n",
    "    Advantages: The exploration of the search space is more effective.\n",
    "    Disadvantages: Time is spent evaluating infeasible solutions. There is a possibility of returning an infeasible solution as the final output of the algorithm.\n",
    "\n",
    "5.4.5.Three strategies for restricted exploration of the feasible region within the search space:\n",
    "    Rejection strategies 拒绝策略: Any infeasible solution generated during the search is directly ignored.\n",
    "    Repair strategies 修复策略: A repair operator is applied to each infeasible solution generated to transform it into a feasible solution. This strategy is often based on heuristics.\n",
    "    Preservation strategies 保存策略: Both the representation scheme and the operators are specifically designed for the problem in a way that ensures the feasibility of generated solutions. It requires more design effort and are problem-specific.\n",
    "\n",
    "5.4.6.Complete exploration of the solution space 完整探索解决方案空间:\n",
    "    The most common scheme for complete exploration of the solution space is penalty-based strategies:\n",
    "        A penalty function is added to the original unconstrained objective function: Min f'(x) = f(x) + w·P(x)\n",
    "        where P(x) is a penalty function and w is a weighting coefficient (intensify/diversify).\n",
    "        P(x) takes a value of 0 when the solution x is feasible. Otherwise, the greater the degree of constraint violation, the larger the value of P\n",
    "\n",
    "5.5.Memoryless and memory-based metaheuristics 无记忆和基于记忆的元启发式\n",
    "5.5.1.Metaheuristics 元启发式\n",
    "    Metaheuristics provide strategies for solving a problem by conducting a search over the space of possible solutions.\n",
    "    The solution representation must include all the necessary information for their identification and evaluation.\n",
    "    A search over a space involves generating a sequence of points in the space, where each point is obtained from the previous one through a series of transformations or movements.\n",
    "    The goal of search-based metaheuristics is to provide guidelines for obtaining paths that yield high-quality solutions while also ensuring adequate effciency\n",
    "5.5.1.1.Memoryless metaheuristics 无记忆元启发式: \n",
    "    do not use or maintain any explicit memory of past search information. They rely solely on the current solution and its neighborhood to make decisions about the next search move. These metaheuristics typically focus on exploration by using randomized or stochastic search techniques.\n",
    "5.5.1.2.Memory-based metaheuristics 基于记忆的元启发式: \n",
    "    are algorithms that use past information or historical data to guide the search process. They remember and store certain aspects of the search, such as the best solutions found so far or promising regions in the solution space. This memory allows them to make informed decisions and adapt their search strategy based on past experience\n",
    "5.5.1.3.Simulated annealing 模拟退火 要考！！！！！！！！！！\n",
    "    is a probabilistic optimisation algorithm inspired by the annealing process in metallurgy. It is used to find near-optimal solutions for combinatorial optimisation problems.\n",
    "5.5.1.4.Tabu search Tabu 搜索\n",
    "is an algorithm used for solving optimisation problems. It is based on the concept of maintaining a tabu list, which keeps track of recently visited solutions to prevent cycling and encourage exploration.\n",
    "\n",
    "5.6.Population-based method 基于人口的方法\n",
    "    Operate on a population of candidate solutions rather than a single solution. The population is typically initialized randomly or using heuristic techniques.\n",
    "    Multiple solutions are evaluated simultaneously, allowing for the exploration of the search space more efficiently.\n",
    "        Advantages 优点: ability to simultaneously explore multiple regions of the search space, promoting diversity and preventing premature convergence to suboptimal solutions.\n",
    "        Disadvantages 缺点: it may require careful parameter tuning and can be computationally demanding due to the population size and iterative nature of the algorithms.\n",
    "    Examples of bio-inspired population-based methods include ant colony optimisation, black hole algorithm, particle swarm optimisation, and genetic algorithms.\n",
    "\n",
    "5.6.1.Swarm optimisation 群体优化\n",
    "    refers to a family of nature-inspired optimisation algorithms that mimic the collective behaviour of decentralised, self-organised systems found in nature (e.g., bird flocks, fish schools, or ant colonies) to find optimal or near-optimal solutions to complex problems.\n",
    "    The main idea is that a population (swarm) of simple agents (particles, ants, bees, etc.) explores the search space collaboratively. Each agent follows simple rules based on its own experience and that of its neighbours, and through many iterations, the swarm converges toward good/reasonable solutions.\n",
    "    The main concepts of swarm optimisation are:\n",
    "        Population-based 基于人口: Many candidate solutions evolve simultaneously.\n",
    "        Decentralised 去中心化: No single agent controls the process, so intelligence emerges from interaction.\n",
    "        Stochastic 随机指标: Uses randomness to explore the search space and avoid local minima.\n",
    "        Adaptive 自适应: Agents adjust their behaviour based on feedback.\n",
    "    Some warm optimisation algorithms:\n",
    "        Particle Swarm Optimisation (PSO) 粒子群优化: Inspired by bird flocking. Each particle adjusts its position based on its own best position and the swarm’s best-known position.\n",
    "        Ant Colony Optimisation (ACO) 蚁群优化: Inspired by how ants find shortest paths. Ants deposit pheromones that guide others toward better solutions.\n",
    "        Artificial Bee Colony (ABC) 人工蜂群: Based on how bees search for food sources and share information.\n",
    "    Applications: function optimisation, robotics path planning, scheduling and resource allocation, neural network training, feature selection in machine learning.\n",
    "\n",
    "5.6.2.Genetic algorithms 遗传算法\n",
    "    Based on Darwin’s evolution theory.\n",
    "    “One general law, leading to the advancement of all organic beings, namely, multiply, vary, let the strongest live and the weakest die.” Charles Darwin.\n",
    "    Stochastic search 随机搜索 technique based on the mechanisms of natural selection and natural genetics.\n",
    "    Use analogies 类比 of natural selection to develop better solutions.\n",
    "    Widely used in problems of nonlinear and high-dimensional optimis\n",
    "    GA model the process of evolution as a sequence of changes in genes, with solutions analogous to chromosomes.\n",
    "    The search space is explored by applying transformations to candidate solutions, just as observed in living organisms: crossover, mutation, and selection\n",
    "    Terminology used in genetic algorithms 遗传算法中使用的术语:\n",
    "\n",
    "5.6.2.1.Three main operators: crossover, mutation, and selection 三个主要算子：交叉、突变和选择\n",
    "        Genetic: crossover and mutation. They emulate the process of gene inheritance to create new solutions. Evolution: selection. It emulates Darwinian evolution to create a population from one generation to another.\n",
    "        Crossover: operates on 2 chromosomes, generating two offspring by combining characteristics. The performance of the algorithm highly depends on this operation.\n",
    "            Crossover rate (p c ): the number of offspring produced each generation divided by the population size.\n",
    "        Mutation: operates on 1 chromosome, producing random spontaneous changes in a gene, contributing to exploration of the search space.\n",
    "            Mutation rate (pm ): percentage of the total number of genes in the population to mutate. It controls the rate at which new genes are introduced into the population.\n",
    "        Selection: the selective pressure is critical for the algorithm.\n",
    "            High pressure: the search may end prematurely (intensification).\n",
    "            Low pressure: progress is slower than necessary (diversification).\n",
    "        The ideal approach is to maintain low pressure at the beginning for broad exploration, and high pressure towards the end to exploit more promising areas\n",
    "5.6.2.2.General structure:\n",
    "    Chromosome commonly represented as strings of bits or binary representation.\n",
    "    Parameters include population size and probability of applying the genetic operators\n",
    "5.6.2.3.New generation size: Uniform\n",
    "    New generation size = Same as previous generation\n",
    "    All offspring and some parents. Originally, all offspring replaced all parents\n",
    "5.6.2.4.New generation size: Expanded\n",
    "    New generation size = Previous generation size + number of offspring\n",
    "    All offspring and parents\n",
    "5.6.2.5.\n",
    "    Stochastic sampling: prevent super chromosomes. For instance, roulette wheel.\n",
    "    Deterministic sampling: sort chromosomes according to their fitness and choose the best ones. Elitist selection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
