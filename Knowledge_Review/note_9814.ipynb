{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c95b42",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf854ab0",
   "metadata": {},
   "source": [
    "1.1.Agent\n",
    "    Reactive Agentååº”ï¼šåŸºäºå½“å‰æ„ŸçŸ¥è¡ŒåŠ¨ï¼Œè·¯å¾„å¯èƒ½æ¬¡ä¼˜. \n",
    "    Model-based AgentåŸºäºæ¨¡å‹ï¼šè·Ÿè¸ªçœ‹ä¸åˆ°äº‹ç‰©æ¥å¤„ç†éƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼Œä½†ä¸èƒ½è®¡åˆ’æœªæ¥ï¼Œå®ç°ä¾‹å¦‚è‡ªåŠ¨é©¾é©¶/æ‰«åœ°æœºå™¨äºº. \n",
    "    Planning Agentè§„åˆ’ï¼šä¸æ¡ä»¶è¡ŒåŠ¨ä¸åŒï¼Œæœ‰æœªæ¥åæœè€ƒè™‘ï¼Œéœ€è¦æœç´¢ï¼Œçµæ´»æ˜“å˜ä½†ååº”æ…¢. \n",
    "        ä¾‹å¦‚ï¼šGoal-based AgentåŸºäºç›®æ ‡ï¼šåœ°å›¾æœç´¢ï¼Œè±¡æ£‹. \n",
    "    Learning Agentå­¦ä¹ ï¼šåŒ…å«performance element:è¯»sensorsæ¥è¡ŒåŠ¨/critic:ç»™åé¦ˆ/Learning element:ç”¨â‘¡ç¡®å®šä¿®æ”¹â‘ /problem generator:è®¾æ–°ä»»åŠ¡ï¼Œç»™ä¿¡æ¯ã€‚ä¼—æ¨¡å—éå­¤ç«‹. \n",
    "  \n",
    "1.2.Searchï¼šç­–ç•¥ä¸åŒåœ¨äºæ‰©å¤§è¾¹ç•Œæ–¹å¼. \n",
    "\n",
    "1.2.1.uninformedï¼š  \n",
    "    BFSå¹¿åº¦ï¼š  \n",
    "    DFSæ·±åº¦ï¼šå †æ ˆï¼ˆé™¤iddfså¤–å…¶ä»–éƒ½é˜Ÿåˆ—ï¼‰. \n",
    "    IDDFSè¿­ä»£æ·±åŒ–æ·±åº¦ï¼šä¸è¶…ç»™å®šæ·±åº¦è¿­ä»£dfs. \n",
    "    UCSç»Ÿä¸€æˆæœ¬ï¼šé€‰pathå¼§çš„å’Œæœ€å°‘ï¼Œæˆæœ¬ç›¸åŒå˜bfs. \n",
    "\n",
    "1.2.2.informedçŸ¥æƒ…æœç´¢ï¼šå¯å‘å¼ï¼Œç”¨domain knowledgeï¼Œæœæœ€ä½³çŒœæµ‹ç›®æ ‡è¡Œ. \n",
    "    GBFSè´ªå©ªï¼šé€‰æœ€ä½heuristic cost. \n",
    "    A*:ç»Ÿä¸€æˆæœ¬+è´ªå©ª  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c188ec0",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfcf276",
   "metadata": {},
   "source": [
    "2.1.Supervised Learning ç›‘ç£å­¦ä¹ \n",
    "    An equation relating input to output\n",
    "    Search through family of possible equations to find one that fits training data well\n",
    "\n",
    "2.1.1.Regressionå›å½’\n",
    "    å•å˜é‡å›å½’é—®é¢˜ï¼ˆä¸€ä¸ªè¾“å‡ºï¼Œå®æ•°å€¼ï¼‰\n",
    "\n",
    "2.1.2.Binary Classification äºŒå…ƒåˆ†ç±»\n",
    "    äºŒå…ƒåˆ†ç±»é—®é¢˜ï¼ˆä¸¤ä¸ªç¦»æ•£ç±»åˆ«ï¼‰\n",
    "\n",
    "2.1.3.Multiclass Classification\n",
    "    å¤šç±»åˆ†ç±»é—®é¢˜ï¼ˆç¦»æ•£ç±»åˆ«ï¼Œ>2 ä¸ªå¯èƒ½çš„å€¼ï¼‰\n",
    "\n",
    "2.2.Unsupervised Learning æ— ç›‘ç£å­¦ä¹ \n",
    "    å­¦ä¹ æ— æ ‡ç­¾æ•°æ®é›†\n",
    "    Clustering èšç±»ï¼šå°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„åœ¨ä¸€èµ·\n",
    "\n",
    "2.3.Reinforcement Learning å¼ºåŒ–å­¦ä¹ \n",
    "    ä¸€ç»„ States çŠ¶æ€/ Actions åŠ¨ä½œ/ Rewards å¥–åŠ±\n",
    "    Goal: take actions to change the state so that you receive rewards\n",
    "    You have to explore the environment yourself to gather data as you go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001099e9",
   "metadata": {},
   "source": [
    "2.4.Decision Trees å†³ç­–æ ‘ï¼ˆsupervised machine learningï¼‰\n",
    "    åº”ç”¨: classification and regression\n",
    "    partition the data into subsets that are increasingly homogeneousåŒè´¨ with respect to the response variable\n",
    "    Feature A may be the most important feature(å®ƒå°†è®­ç»ƒæ ·æœ¬åˆ’åˆ†ä¸ºæ›´æ¥è¿‘å®Œå…¨æ­£é¢æˆ–å®Œå…¨è´Ÿé¢çš„å­é›†ï¼ˆå³æ›´åŒè´¨åŒ–ï¼‰)ï¼Œso check it earlier(å€’ç€ç”»)\n",
    "    Goal:generalizesæ³›åŒ– well from the training data and accurately classifies previously unseen sampleså‡†ç¡®åˆ†ç±»å…ˆå‰æœªè§è¿‡çš„æ ·æœ¬\n",
    "\n",
    "2.4.1.Entropy ç†µ (randomness or uncertainty éšæœºæ€§æˆ–ä¸ç¡®å®šæ€§)\n",
    "    ğ»(âŸ¨p1,Â· Â· Â· , pn âŸ©) = sigma(i=1,n) âˆ’pğ‘–log2ğ‘ğ‘–\n",
    "    maximized when all outcomes are equally likely\n",
    "    minimized when the probability distribution is highly concentrated around a single outcome\n",
    "    å†³ç­–æ ‘é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é€‰æ‹©ã€æœ€å°ã€‘ç†µçš„ç‰¹å¾æ¥æ„å»º\n",
    "\n",
    "2.4.2.Minimal Error Pruning æœ€å°é”™è¯¯å‰ªæ\n",
    "    prune branches that do not provide much benefit in classifying the items (aids generalization, avoids overfitting)\n",
    "    Laplace error æ‹‰æ™®æ‹‰æ–¯è¯¯å·®:E= 1âˆ’(n+1)/(N+k)\n",
    "        N = total number of (training) items at the node\n",
    "        n = number of (training) items in the majority class\n",
    "        k = number of classes\n",
    "    å¦‚æœå­èŠ‚ç‚¹çš„å¹³å‡æ‹‰æ™®æ‹‰æ–¯è¯¯å·®è¶…è¿‡çˆ¶èŠ‚ç‚¹ï¼Œåˆ™å‰ªé™¤å­èŠ‚ç‚¹\n",
    "\n",
    "2.4.3.Tree Limitations\n",
    "    high varianceé«˜æ–¹å·®(ä¸ç¨³å®š): 1.arise from a tiny change in the training data, leading to completely different predictions\n",
    "                               2.æ¨¡å‹å€¾å‘äºâ€œè®°å¿†â€è®­ç»ƒæ•°æ®ï¼ˆoverfitting è¿‡æ‹Ÿåˆï¼‰ï¼Œè€Œä¸æ˜¯å­¦ä¹ æ½œåœ¨çš„æ¨¡å¼\n",
    "                               3.ä½æ–¹å·®: æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šæ›´åŠ ç¨³å®šå’Œä¸€è‡´\n",
    "    Suboptimal predictive performance é¢„æµ‹æ€§èƒ½ä¸ä½³: Single trees often fail to achieve strong generalization æ³›åŒ–èƒ½åŠ›\n",
    "    generalization æ³›åŒ– æ˜¯æŒ‡æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè€Œä¸ä»…ä»…æ˜¯è®­ç»ƒæ•°æ®\n",
    "    divided into three subsets: training, validation,and test\n",
    "                              1.è®­ç»ƒé›†ï¼šç”¨äºè®­ç»ƒå†³ç­–æ ‘æ¨¡å‹ã€‚\n",
    "                              2.éªŒè¯é›†ï¼šç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°ä¸åŒçš„é…ç½®ï¼ˆè¶…å‚æ•°ï¼‰\n",
    "                              3.æµ‹è¯•é›†ï¼šä¸ºæœ€ç»ˆå†³ç­–æ ‘æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šæä¾›æ— åè¯„ä¼°ã€‚\n",
    "    Bias åå·®: systematic error ç³»ç»Ÿæ€§è¯¯å·®, ç”¨ç®€å•çš„æ¨¡å‹è¿‘ä¼¼ç°å®é—®é¢˜æ—¶å¼•å…¥çš„\n",
    "        A high bias model makes strong assumptions about the data, leading to underfittingæ¬ æ‹Ÿåˆ\n",
    "    bias-variance tradeoff:å¤æ‚çš„æ¨¡å‹å¯ä»¥å¸®åŠ©æˆ‘ä»¬é¿å…åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼Œè€Œé™åˆ¶æ¨¡å‹å¤æ‚åº¦å¯ä»¥å¸®åŠ©æˆ‘ä»¬é¿å…è¿‡æ‹Ÿåˆï¼ˆé«˜æ–¹å·®ï¼‰\n",
    "\n",
    "2.4.4.Tree Depth\n",
    "    Shallow trees æµ…æ ‘ç”±äºæ ‘æ²¡æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹æ¥æ•æ‰æ•°æ®çš„å¤æ‚æ€§ï¼Œå› æ­¤ä¼šé­å—é«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰\n",
    "    Deep trees æ·±æ ‘å®¹æ˜“å—åˆ°é«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰çš„å½±å“ï¼Œå› ä¸ºæ ‘å¯èƒ½ä¼šä¸ºæ¯ä¸ªæ•°æ®æ ·æœ¬åˆ†é…ä¸€ä¸ªç‹¬ç‰¹çš„è·¯å¾„ï¼Œè€Œä¸æ˜¯å­¦ä¹ ä¸€èˆ¬æ¨¡å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad0483",
   "metadata": {},
   "source": [
    "2.5.Ensemble Learning with Trees åŸºäºæ ‘çš„é›†æˆå­¦ä¹ \n",
    "2.5.1.Bagged Tree è£…è¢‹æ ‘\n",
    "    relies on bootstrapping è‡ªåŠ©é‡‡æ · to create multiple training datasets\n",
    "    bootstrapping: with replacement æœ‰æ”¾å›, sample is the same size è§„æ¨¡ç›¸åŒ as the original dataset\n",
    "        37% out-of-bag (OOB) samples: æœªåŒ…å«åœ¨è‡ªåŠ©é‡‡æ ·ä¸­çš„æ•°æ®ç‚¹\n",
    "    é›†æˆä¸­çš„æ¯ä¸ªæ¨¡å‹éƒ½ä¼šä¸ºæ–°æ ·æœ¬ç”Ÿæˆä¸€ä¸ªé¢„æµ‹ï¼Œæœ€ç»ˆé¢„æµ‹æ˜¯é€šè¿‡the majority vote ruleå¤šæ•°æŠ•ç¥¨è§„åˆ™ï¼ˆç”¨äºåˆ†ç±»ï¼‰æˆ–é€šè¿‡å¹³å‡é¢„æµ‹å€¼ï¼ˆç”¨äºå›å½’ï¼‰æ¥åšå‡ºçš„\n",
    "    Advantages:1.reduces the variance\n",
    "               2.average prediction has lower variance\n",
    "               3.errors(vary in different directions) partially cancel out\n",
    "               4.No separate test set required\n",
    "               5.OOB samples provides an unbiased estimate of performance æ— åçš„æ€§èƒ½ä¼°è®¡\n",
    "    æ¯æ£µå­æ ‘æœ€ç»ˆéƒ½æ˜¯ç‹¬ä¸€æ— äºŒçš„,ç„¶è€Œï¼Œè¿™äº›æ ‘å½¼æ­¤ä¹‹é—´ä»ç„¶å­˜åœ¨ä¸€å®šçš„correlatedç›¸å…³æ€§,å› æ­¤ï¼Œè£…è¢‹æ–¹æ³•å®ç°çš„æ–¹å·®å‡å°‘å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›ï¼Œä»ç»Ÿè®¡å­¦çš„è§’åº¦æ¥çœ‹ï¼Œé€šè¿‡åœ¨æ ‘æ„å»ºè¿‡ç¨‹ä¸­å¼•å…¥additional randomnessé¢å¤–çš„éšæœºæ€§ï¼Œå¯ä»¥å‡å°‘é¢„æµ‹å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§\n",
    "\n",
    "2.5.2.Random Forest éšæœºæ£®æ—\n",
    "    æœ€ç»ˆé¢„æµ‹: the majority vote rule\n",
    "    åœ¨æ¯ä¸ªæ ‘çš„åˆ†å‰²ç‚¹ï¼Œç®—æ³•éšæœºé€‰æ‹© k predictors (features) k ä¸ªé¢„æµ‹å™¨ï¼ˆç‰¹å¾ï¼‰ã€‚k = âˆšpï¼Œå…¶ä¸­ p æ˜¯ç‰¹å¾çš„æ€»æ•°\n",
    "    ç„¶åï¼Œæ ‘ä»…ä»è¿™äº› k ä¸ªç‰¹å¾ä¸­é€‰æ‹©æœ€ä½³åˆ†å‰²\n",
    "    è¿™ç§éšæœºæ€§å‡å°‘äº†æ ‘ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆè¿™æ˜¯è£…è¢‹æ ‘çš„é—®é¢˜ï¼‰\n",
    "    ä¸è£…è¢‹æ³•ç›¸æ¯”ï¼Œéšæœºæ£®æ—åœ¨é€æ£µæ ‘çš„åŸºç¡€ä¸Šæ›´å…·æœ‰è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºæ ‘æ„å»ºè¿‡ç¨‹ä»…éœ€åœ¨æ¯ä¸ªåˆ†è£‚ç‚¹è¯„ä¼°a fraction of the original featuresåŸå§‹ç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼Œå°½ç®¡éšæœºæ£®æ—é€šå¸¸éœ€è¦more treesæ›´å¤šçš„æ ‘\n",
    "    é€‰æ‹©éšæœºæ£®æ—å’Œ Bagging ä¸­æ ‘çš„æ•°é‡ï¼ˆmï¼‰ï¼š1.Variance reduction æ–¹å·®å‡å°‘: å¢åŠ æ›´å¤šçš„æ ‘å¯ä»¥å‡å°‘æ–¹å·®ï¼Œä½¿é¢„æµ‹æ›´åŠ ç¨³å®š\n",
    "                                      2.Constraints çº¦æŸæ¡ä»¶: å½“å¢åŠ  m æ—¶ï¼Œè®­ç»ƒæ—¶é—´ã€å†…å­˜ä½¿ç”¨å’Œè¿‡æ‹Ÿåˆæ˜¯ä¸»è¦çš„é™åˆ¶å› ç´ \n",
    "                                      3.Test error behavior æµ‹è¯•è¯¯å·®è¡Œä¸º: æµ‹è¯•è¯¯å·®é€šå¸¸éšç€ m çš„å¢åŠ è€Œå•è°ƒé€’å‡â€”â€”èµ·åˆè¿…é€Ÿä¸‹é™ï¼Œç„¶åè¶‹äºå¹³ç¨³ï¼Œåœ¨è¶³å¤Ÿå¤šçš„æ ‘ä¹‹åå‡ ä¹ä¿æŒæ’å®š\n",
    "\n",
    "2.5.3.Boosting Trees(AdaBoost)æå‡æ ‘\n",
    "    æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒAdaBoost æ ¹æ®å½“å‰çš„åŠ æƒæ•°æ®ç‚¹é€‰æ‹©æœ€ä½³åˆ†ç±»å™¨\n",
    "    åœ¨ k æ¬¡è¿­ä»£ä¸­è¢«é”™è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹åœ¨(k+1)æ¬¡è¿­ä»£ä¸­è·å¾—higher weightsæ›´é«˜çš„æƒé‡\n",
    "    å› æ­¤ï¼Œéš¾ä»¥åˆ†ç±»çš„æ ·æœ¬ä¼šé€æ¸è·å¾—è¶Šæ¥è¶Šå¤§çš„æƒé‡\n",
    "    è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿æ¯æ¬¡è¿­ä»£éƒ½ä¸“æ³¨äºå­¦ä¹ æ•°æ®çš„æŸä¸ªä¸åŒæ–¹é¢\n",
    "    æœ€åï¼Œè¿™äº›weighted classifiersåŠ æƒåˆ†ç±»å™¨çš„åºåˆ—è¢«ç»„åˆæˆä¸€ä¸ªé›†æˆï¼Œäº§ç”Ÿä¸€ä¸ªå¼ºå¤§çš„æ•´ä½“æ¨¡å‹\n",
    "    Algorithm:è®¡ç®—\n",
    "\n",
    "\n",
    "2.5.4.å¯¹æ¯”\n",
    "    è£…è¢‹æ³•é€šè¿‡åœ¨è‡ªåŠ©é‡‡æ ·ä¸Šè®­ç»ƒå¤šä¸ªæ ‘å¹¶èšåˆå®ƒä»¬çš„é¢„æµ‹æ¥é™ä½high varianceé«˜æ–¹å·®\n",
    "    éšæœºæ£®æ—é€šè¿‡åœ¨è®­ç»ƒä¸­æ·»åŠ randomnesséšæœºæ€§ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ¯ä¸ªåˆ†è£‚æ—¶é€‰æ‹©éšæœºç‰¹å¾å­é›†ï¼‰è¿›ä¸€æ­¥variance reductionå‡å°‘æ–¹å·®.æ ‘æ˜¯in parallelå¹¶è¡Œä¸”åŒç­‰æƒé‡è®­ç»ƒçš„ï¼Œè¿™èƒ½æœ‰æ•ˆå‡å°‘æ–¹å·®ï¼Œä½†å¹¶ä¸èƒ½æ˜¾è‘—reduce biaså‡å°‘åå·®\n",
    "    æå‡æ–¹æ³•é€šè¿‡sequentiallyé¡ºåºè®­ç»ƒæ ‘æ¥å‡å°‘bias and varianceåå·®å’Œæ–¹å·®ï¼Œå…¶ä¸­æ¯æ£µæ–°æ ‘ä¸“æ³¨äºçº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ï¼Œå¹¶æ ¹æ®å…¶æ€§èƒ½å¯¹æ ‘è¿›è¡ŒåŠ æƒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45199a53",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93278383",
   "metadata": {},
   "source": [
    "3.1.Artificial Neural Networksäººå·¥ç¥ç»ç½‘ç»œ\n",
    "    This model consists of many interconnected processing units (neurons)ç¥ç»å…ƒ that work in parallel to accomplish a global task\n",
    "    model relationships between inputs and outputs or to discover patterns in data\n",
    "    Learning occurs by adapting weights, architecture, and activation/transfer functions to improve performance\n",
    "    Characterized by: Number of neurons, Interconnection architecture, Weight values, Activation and transfer functions\n",
    "\n",
    "3.1.1.model of a neuron\n",
    "    An extra constant w0 called the biasåå·® is also added\n",
    "\n",
    "3.1.2.Neural Networks are made up of nodes which have:\n",
    "    Input edges, each with some weight\n",
    "    Output edges (with weights)\n",
    "    An activation level (a function of the inputs)\n",
    "        Weights can be positive or negative and may change overtime (learning).\n",
    "        The input function is the weighted sum of the activation levels of inputs.\n",
    "        The activation level is a non-linear transfer function (activation function), g, of this input.\n",
    "        z= g(s)= g(ğ‘¤0 + Ïƒğ‘– ğ‘¤ğ‘–ğ‘¥ğ‘–) å¿…é¡»å¤§äº0æ‰æ˜¯1\n",
    "\n",
    "3.2.Perceptronæ„ŸçŸ¥å™¨\n",
    "    An artificial neuron with step transfer functioné˜¶è·ƒä¼ é€’å‡½æ•°(é›¶ç‚¹å·¦å³0,1) is called a Perceptron\n",
    "    The bias (w0) was thought of as a kind of thresholdé˜ˆå€¼. If the combination -Ïƒğ‘– ğ‘¤ğ‘–ğ‘¥ğ‘– is less than this threshold, the neuron would \"fire\" (its output would be 1)\n",
    "    The higher the bias, the more likely the neuron is to fire\n",
    "    Later on, alternative transfer functions were introduced which are continuous and (mostly) differentiable\n",
    "3.2.1.hyperplane\n",
    "    The weights and bias of a Perceptron define a hyperplaneè¶…å¹³é¢ that divides the input space into two regions\n",
    "        For inputs on one side of the hyperplane, the output is 0\n",
    "        For inputs on the other side, the output is 1\n",
    "    Functions that can be computed in this way are called linearly separableçº¿æ€§å¯åˆ†. With this structure, a perceptron (artificial neuron) can learn any linear relationshipä»»ä½•çº¿æ€§å…³ç³»\n",
    "3.2.2.Application\n",
    "    Limitation: What happens if the data is not linearly separable\n",
    "    AND, OR, and NOT become linearly separable,but XOR are not\n",
    "\n",
    "3.3.Multi Layer Perceptronå¤šå±‚æ„ŸçŸ¥å™¨\n",
    "    compute XOR, one approach is to rewrite it in terms of linearly separable functions such as AND, OR, and NOR, and then arrange several perceptronå¤šä¸ªæ„ŸçŸ¥å™¨æ’åˆ— into a network that combines these functions appropriately\n",
    "    However, in practice, we usually deal with raw dataåŸå§‹æ•°æ® rather than explicit logical expressions\n",
    "    perceptron learning algorithmâ€”that can learn the weights of a neural networkäº†è§£ç¥ç»ç½‘ç»œçš„æƒé‡ from a set of training examples\n",
    "\n",
    "3.4.Learning with Gradient Descent and Backpropagationé€šè¿‡æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­è¿›è¡Œå­¦ä¹ \n",
    "    to optimize over a family of continuous and differentiable functions\n",
    "    define an error functionè¯¯å·®å‡½æ•° (also called a loss function or cost function) ğ¸as half the sum, over all input items, of the square of the difference between the actual output ğ‘§ğ‘–and the target output ğ‘¡ğ‘–\n",
    "    goal is to find the minimum of the error function ğ¸æ‰¾åˆ°è¯¯å·®å‡½æ•° E çš„æœ€å°å€¼, and minimum of a function can be calculated through its derivativeå¯¼æ•°\n",
    "    We can use multi-variable derivative to adjust the weights in such a way as to take us in the steepest downhill direction. w â† wâˆ’ Î· ğğ‘¬/ğğ’˜ , Î· is called the learning rateå­¦ä¹ ç‡\n",
    "    If we use the step function as the transfer function, the error landscape is not smooth; it consists almost entirely of flat regions and \"shoulders,\" with occasional discontinuous jumps.\n",
    "    For a single-layer perceptron, this was not a problem, but for networks with two or more layers, it becomes a major obstacle.\n",
    "    To apply gradient descent successfully, neural networks needed to be redesigned so that the function from input to output would be smooth and differentiableå¹³æ»‘ä¸”å¯å¾®\n",
    "3.4.1.Learning rate identify the step sizeå­¦ä¹ ç‡ç¡®å®šæ­¥é•¿\n",
    "    The key idea is to replace the (discontinuous) step function with a differentiable function, such as the sigmoid or hyperbolic tangent g(s)= ğŸ/ğŸ+ğ’†^(âˆ’ğ’”)\n",
    "    We now describe how to compute the partial derivatives of the loss function with respect to each weightæŸå¤±å‡½æ•°ç›¸å¯¹äºæ¯ä¸ªæƒé‡çš„åå¯¼æ•°\n",
    "    For illustration, we consider a two-layer neural network with sigmoid activationæ¿€æ´» sigmoid çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ at the hidden layer, as shown in the diagram\n",
    "    ğ‘¥ğ‘– are the inputs, ğ‘¦ğ‘– are the hidden units, ğ‘¤ğ‘–ğ‘— and ğ‘£ğ‘– are the weights, and ğ‘ğ‘– and ğ‘ are the biases\n",
    "    Chain Rule(ä¼šè€ƒ)\n",
    "\n",
    "3.4.2.Backpropagationåå‘ä¼ æ’­\n",
    "    Forward passå‰å‘ä¼ é€’: apply inputs to the â€œlowest layerâ€ and feed activations forward to get output\n",
    "    Calculate errorè®¡ç®—è¯¯å·®: difference between desired output and actual output \n",
    "    Backward passå‘åä¼ é€’: Propagate errors back through the network to adjust weights\n",
    "    Since our prediction is greater than the target value, we need to decrease the weighté™ä½æƒé‡ in order to reduce the prediction in the next iterations\n",
    "    As we propagate back to the weights in the earlier layers, the amount of adjustment (decrement) becomes smallerå˜å°. For example, the change in ğ‘¤13 is more significant than the change in ğ‘¤11\n",
    "\n",
    "3.5.Neural Networks Designç¥ç»ç½‘ç»œè®¾è®¡\n",
    "\n",
    "3.5.1.Exhaustive Analysis of the Systemå¯¹ç³»ç»Ÿçš„è¯¦å°½åˆ†æ\n",
    "    Is a neural network really the best solution for this problem? Do I have the necessary requirements?ç¥ç»ç½‘ç»œçœŸçš„æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„æœ€ä½³è§£å†³æ–¹æ¡ˆå—ï¼Ÿæˆ‘æœ‰å¿…è¦çš„è¦æ±‚å—ï¼Ÿ\n",
    "    Neural Networks: The second-best optionç¥ç»ç½‘ç»œï¼šç¬¬äºŒå¥½çš„é€‰æ‹©\n",
    "        Neural networks are highly data-sensitive and usually require large datasets, since they need to learn a large number of parameters (ğ‘¤ğ‘–)\n",
    "        Collecting and preparing such data can be costly and time-consuming\n",
    "\n",
    "3.5.2.Preprocessingé¢„å¤„ç†\n",
    "    What steps should I take before feeding data into the networkåœ¨å°†æ•°æ®è¾“å…¥ç½‘ç»œä¹‹å‰ï¼Œæˆ‘åº”è¯¥é‡‡å–å“ªäº›æ­¥éª¤ï¼Ÿ\n",
    "    Dataæ•°æ®:\n",
    "        A neural network is essentially a black-box model designed for interpolation (with no guarantee of good performance in extrapolation).\n",
    "        Therefore, its effectiveness strongly depends on the quality and quantity of the data available.\n",
    "    Qualityè´¨é‡:\n",
    "        This refers to how well the available data represents the underlying function being approximated.\n",
    "    Quantityæ•°é‡:\n",
    "        Only with a sufficiently large dataset can we expect to correctly identify the parameters (weights) of a neural model.\n",
    "        If the available data is insufficient, data augmentation techniques can be used to expand it\n",
    "3.5.2.1.Data cleaning:\n",
    "    Detect and, if possible, eliminate outliers, empty values, etc. It might also help to detect correlations between variables.\n",
    "    Normalisation of variables:\n",
    "        Xn = (X- Xmin)/ (Xmax-Xmin); Xn âˆˆ [0,1] or Xn = 2*(X-Xmin)/(Xmax-Xmin) â€“ 1; Xn âˆˆ [-1,1]\n",
    "    It is necessary to perform the corresponding denormalization at the output stageè¾“å‡ºé˜¶æ®µ\n",
    "3.5.2.2.why data normalizationä¸ºä»€ä¹ˆè¦è¿›è¡Œæ•°æ®å½’ä¸€åŒ–\n",
    "    1. To ensure features are comparable in scale ç¡®ä¿åŠŸèƒ½åœ¨è§„æ¨¡ä¸Šå…·æœ‰å¯æ¯”æ€§\n",
    "        Many datasets include features with very different ranges (e.g., age in years vs. income in dollars).\n",
    "        Without normalization, features with larger scales can dominate smaller ones in distance-based or gradient-based models.\n",
    "    2. To improve training stability and speed æé«˜è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦\n",
    "        Gradient descent converges faster when features are on a similar scale.\n",
    "        If not normalized, the loss landscape can become very steep in some directions and flat in others.\n",
    "\n",
    "3.5.3.Design of the Neural Network ç¥ç»ç½‘ç»œçš„è®¾è®¡\n",
    "    What should the architecture of my network look like? (e.g., number of layers, number of neurons per layer, choice of activation functions, and other hyperparameters). æˆ‘çš„ç½‘ç»œæ¶æ„åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿï¼ˆä¾‹å¦‚ï¼Œå±‚æ•°ã€æ¯å±‚ç¥ç»å…ƒæ•°ã€æ¿€æ´»å‡½æ•°çš„é€‰æ‹©å’Œå…¶ä»–è¶…å‚æ•°ï¼‰\n",
    "    Rule of thumbç»éªŒæ³•åˆ™: Nh should lead to a number of parameters (weights) Nw that: Nw < (Number of samples) / 10\n",
    "        The number of weights Nw of a MLP, with Ni neurons in its input layer, a hidden layer with Nh neurons, and No neurons in the output layer is: Nw = (Ni+1)*Nh+(Nh+1)*No\n",
    "    Conclusion: more parameters and deeper networks do not lead to better a performance necessarily.\n",
    "    There should be a proportion between your dataset size, problem complexity, etc. and number of parameters æ•°æ®é›†å¤§å°ã€é—®é¢˜å¤æ‚æ€§ç­‰ä¸å‚æ•°æ•°é‡ä¹‹é—´åº”è¯¥æœ‰ä¸€å®šæ¯”ä¾‹\n",
    "    In MLPs is demonstrated that using one hidden layer with a proper number of neurons is sufficient to approximate any non-linear function with an arbitrary precision degree (Universal Approximation Theorem)åœ¨ MLP ä¸­ï¼Œè¯æ˜ä½¿ç”¨å…·æœ‰é€‚å½“æ•°é‡ç¥ç»å…ƒçš„éšè—å±‚å°±è¶³å¤Ÿä»¥ä»»æ„ç²¾åº¦åº¦è¿‘ä¼¼ä»»ä½•éçº¿æ€§å‡½æ•°ï¼ˆé€šç”¨é€¼è¿‘å®šç†ï¼‰ã€‚\n",
    "    Activation functionsæ¿€æ´»å‡½æ•°:\n",
    "        A usual criterion is to use sigmoid functions or ReLUs in the hidden layer and linear functions in the output. However, sigmoids or softmax can also be used in the output.\n",
    "\n",
    "3.5.4.Training\n",
    "    What happens during the training phase?\n",
    "    Training a neural network is a hard process due to the complexity of the error function solution space, which can have numerous local minima, minimax points, etc\n",
    "    There are three main problems that can arise during training:\n",
    "        Biasåå·®\n",
    "        Overparameterizationè¿‡åº¦å‚æ•°åŒ–\n",
    "        Overfittingè¿‡æ‹Ÿåˆ\n",
    "    The latter two might affect the network's ability to generalize (high variance)æ³›åŒ–èƒ½åŠ›ï¼ˆé«˜æ–¹å·®ï¼‰\n",
    "3.5.4.1.Bias\n",
    "    refers to the systematic error introduced by approximating a real-world problem (which may be very complex) with a simpler model.åå·®æ˜¯æŒ‡é€šè¿‡ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹è¿‘ä¼¼ç°å®ä¸–ç•Œçš„é—®é¢˜ï¼ˆå¯èƒ½éå¸¸å¤æ‚ï¼‰è€Œå¼•å…¥çš„ç³»ç»Ÿè¯¯å·®ã€‚\n",
    "    A high bias model makes strong assumptions about the data, leading to underfitting.é«˜åå·®æ¨¡å‹å¯¹æ•°æ®åšå‡ºå¼ºçƒˆçš„å‡è®¾ï¼Œå¯¼è‡´æ‹Ÿåˆä¸è¶³ã€‚\n",
    "    It canâ€™t capture the true complexity of the underlying relationshipå®ƒæ— æ³•æ•æ‰æ½œåœ¨å…³ç³»çš„çœŸæ­£å¤æ‚æ€§ã€‚\n",
    "    To Decrease Bias:\n",
    "        One way to reduce bias is to run multiple training processes starting from different randomly chosen initial weights (e.g., 20 or more attempts). This increases the chance of reaching a better local minimum.\n",
    "        Another approach is to increase the number of neurons in the hidden layer, which allows the model to better capture the complexity of the problem.\n",
    "            However, there is a trade-off: adding too many neurons can lead to high variance and overparameterization, causing the model to overfitæ·»åŠ è¿‡å¤šçš„ç¥ç»å…ƒä¼šå¯¼è‡´é«˜æ–¹å·®å’Œè¿‡åº¦å‚æ•°åŒ–ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚\n",
    "3.5.4.2.Overfittingè¿‡æ‹Ÿåˆ\n",
    "    A model is considered overparameterized when it has more trainable parameters than the available training data can uniquely determine.\n",
    "    In simpler terms, the model has too many parameters relative to the size or complexity of the dataset.\n",
    "    As a result, it becomes powerful enough to memorizeè®°ä½ the training data (including noise) rather than learning the underlying general patterns\n",
    "    The neural network is trained by presenting it with the input and target values for all items in the training set\n",
    "    After completing the learning procedure, it must then predict the outputs for items in the test set\n",
    "    The goal is to accurately predict the target values for the test set based on the input attributes.\n",
    "    A common mistake to avoid is building a model that fits the training data very well but performs poorly on unseen test dataâ€”this problem is known as overfitting.è¿‡åº¦æ‹Ÿåˆ\n",
    "    In contrast, a model that achieves high accuracy on both the training and test sets is said to have good generalizationæ¦‚æ‹¬æ€§\n",
    "    To determine the optimal model parameters, the dataset is often divided into Training, Validation, and Test (generalization) setsä¸ºäº†ç¡®å®šæœ€ä½³æ¨¡å‹å‚æ•°ï¼Œæ•°æ®é›†é€šå¸¸åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•ï¼ˆæ³›åŒ–ï¼‰é›†ã€‚\n",
    "    Typically, as the number of hidden nodes increases, both the training and test errors initially decrease.\n",
    "    However, beyond a certain point, the training error may continue to decrease while the test error plateaus or even slightly increases. The approximate number of hidden nodes at which the test error is minimal is likely to achieve the best generalization performanceæµ‹è¯•è¯¯å·®æœ€å°çš„éšè—èŠ‚ç‚¹çš„è¿‘ä¼¼æ•°é‡å¯èƒ½ä¼šå®ç°æœ€ä½³çš„æ³›åŒ–æ€§èƒ½\n",
    "3.5.4.3.Conclusion:\n",
    "    Increasing the number of neurons can help reduce bias (underfitting), while limiting the number of neurons helps prevent overfitting (high variance). This balance reflects the well-known biasâ€“variance trade-off.å¢åŠ ç¥ç»å…ƒæ•°é‡æœ‰åŠ©äºå‡å°‘åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼Œè€Œé™åˆ¶ç¥ç»å…ƒæ•°é‡æœ‰åŠ©äºé˜²æ­¢è¿‡åº¦æ‹Ÿåˆï¼ˆé«˜æ–¹å·®ï¼‰ã€‚è¿™ç§å¹³è¡¡åæ˜ äº†ä¼—æ‰€å‘¨çŸ¥çš„åå·®-æ–¹å·®æƒè¡¡ã€‚\n",
    "    In addition to adjusting the network size, there are other techniques available to address\n",
    "underfitting and overfitting, such as regularization, early stopping, and data augmentation.\n",
    "\n",
    "3.5.5.Testing and Evaluationæµ‹è¯•å’Œè¯„ä¼°\n",
    "    How should I evaluate the performance of my network\n",
    "    To test the generalization capability of the network, performance is evaluated on the test setåœ¨æµ‹è¯•é›†\n",
    "    Some common loss functions:\n",
    "        Mean Squared Error (MSE):\n",
    "        Mean Absolute Error (MAE):\n",
    "        Cross-Entropy (Binary Cross-Entropy (for 2 classes))\n",
    "\n",
    "3.6.Deep Learning æ·±åº¦å­¦ä¹ \n",
    "    If we denote the first layer as ğ‘“ 1 , the second layer is ğ‘“ 2 , and so on. The total number of layers defines the depthæ·±åº¦ of the model, which is why this approach is known as deep learningæ·±åº¦å­¦ä¹ \n",
    "3.6.1.depth\n",
    "    The depth of a model is defined as the total number of layers with learnable parameters, excluding theinput layer.ä¸åŒ…æ‹¬è¾“å…¥å±‚\n",
    "    Deeper networks can typically approximate more complex functions, while shallower models have fewer layers and are limited to approximating simpler functions.\n",
    "\n",
    "Deep learning is based on the philosophy of connectionism:è¿æ¥ä¸»ä¹‰\n",
    "    while an individual biological neuron is not particularly intelligent, a large population of neurons or features acting together can exhibit intelligent behavior\n",
    "    It is important to emphasize that the number of neurons must be large.ç¥ç»å…ƒçš„æ•°é‡å¿…é¡»å¾ˆå¤§\n",
    "    One of the key factors behind the dramatic improvement in neural network accuracy and the complexity of tasks they can handleâ€”from the 1980s to todayâ€”is the significant increase in network size\n",
    "\n",
    "    Deep learning refers to artificial neural networks with multiple layers that automatically learn\n",
    "patterns and representations from data\n",
    "    What makes a network â€œdeepâ€ is the presence of many layers of neurons, not just one or two. A shallow networkæµ…å±‚ç½‘ç»œ might have only a single hidden layer, whereas a deep networkæ·±åº¦ç½‘ç»œ can have hundreds or even thousands of layers\n",
    "\n",
    "    Universal Approximation Theoremé€šç”¨é€¼è¿‘å®šç†\n",
    "        One layer with many neurons:\n",
    "            wide hidden layeréå¸¸å®½çš„éšè—å±‚,\n",
    "            but it is still shallowæ·±åº¦è¾ƒä½ \n",
    "            computationally inefficient and does not capture hierarchical representations effectively (low precision)\n",
    "        Many layers, each with one neuron:\n",
    "            This network is extremely deep but very narrow.éå¸¸æ·±ï¼Œä½†éå¸¸ç‹­çª„\n",
    "            It qualifies as deep learningæ·±åº¦å­¦ä¹  because the depth (number of layers) is enormous.\n",
    "            However, with only one neuron per layer, the networkâ€™s expressive power is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c36d9",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdedcd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38c218e",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
